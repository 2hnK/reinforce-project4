# PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‹¤í—˜ ì„¤ê³„

**Student ID:** 20227128 ê¹€ì§€í›ˆ  
**Date:** 2025-11-11

---

## ğŸ”’ ê³ ì • íŒŒë¼ë¯¸í„° (ë³€ê²½ ë¶ˆê°€)

```python
lambda_ = 0.95                 # GAE Lambda
lr = 0.0003                    # Learning Rate
num_epochs = 15                # Training Epochs
train_batch_size = 32 * 512    # 16384
minibatch_size = 4096          # Minibatch Size
vf_loss_coeff = 0.01           # Value Function Loss Coefficient
fcnet_hiddens = [64, 64]       # Network Architecture
fcnet_activation = "tanh"      # Activation Function
vf_share_layers = False        # Separate Value/Policy Networks
```

---

## âœ… ì‹¤í—˜ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°

### 1. **PPO Clip Parameter** (`clip_param`)
**ê¸°ë³¸ê°’:** 0.2  
**ê³µì‹ ë¬¸ì„œ:** PPOì˜ í•µì‹¬ íŒŒë¼ë¯¸í„°. ì •ì±… ì—…ë°ì´íŠ¸ì˜ í´ë¦¬í•‘ ë²”ìœ„

**ì œì•ˆ ì‹¤í—˜ê°’:**
- **0.1** (ë³´ìˆ˜ì ): ì•ˆì •ì ì´ì§€ë§Œ ëŠë¦° ìˆ˜ë ´
- **0.2** (ê¸°ë³¸ê°’)
- **0.3** (ê³µê²©ì ): ë¹ ë¥¸ ìˆ˜ë ´, ë¶ˆì•ˆì • ê°€ëŠ¥

**ì˜ˆìƒ íš¨ê³¼:**
- ë‚®ì€ ê°’ â†’ ì •ì±… ë³€í™” ì œí•œ, ì•ˆì •ì  í•™ìŠµ
- ë†’ì€ ê°’ â†’ í° ì •ì±… ì—…ë°ì´íŠ¸ í—ˆìš©, ë¹ ë¥¸ í•™ìŠµ

**ì½”ë“œ:**
```python
config.training(clip_param=0.1)  # or 0.3
```

---

### 2. **Value Function Clip Parameter** (`vf_clip_param`)
**ê¸°ë³¸ê°’:** 10.0 (ë˜ëŠ” None)  
**ê³µì‹ ë¬¸ì„œ:** ê°€ì¹˜ í•¨ìˆ˜ ì†ì‹¤ì— ëŒ€í•œ í´ë¦¬í•‘

**ì œì•ˆ ì‹¤í—˜ê°’:**
- **None** (í´ë¦¬í•‘ ì—†ìŒ)
- **1.0** (ì‘ì€ í´ë¦¬í•‘)
- **10.0** (ê¸°ë³¸ê°’)
- **100.0** (í° í´ë¦¬í•‘)

**ì˜ˆìƒ íš¨ê³¼:**
- None â†’ ì œí•œ ì—†ëŠ” ê°€ì¹˜ í•¨ìˆ˜ ì—…ë°ì´íŠ¸
- ì‘ì€ ê°’ â†’ ê°€ì¹˜ í•¨ìˆ˜ ë³€í™” ì œí•œ
- í° ê°’ â†’ ë” ììœ ë¡œìš´ ê°€ì¹˜ ì¶”ì •

**ì½”ë“œ:**
```python
config.training(vf_clip_param=1.0)
```

---

### 3. **Entropy Coefficient** (`entropy_coeff`)
**ê¸°ë³¸ê°’:** 0.0  
**ê³µì‹ ë¬¸ì„œ:** íƒí—˜ì„ ì¥ë ¤í•˜ëŠ” ì—”íŠ¸ë¡œí”¼ ì •ê·œí™”

**ì œì•ˆ ì‹¤í—˜ê°’:**
- **0.0** (ê¸°ë³¸ê°’, íƒí—˜ ì—†ìŒ)
- **0.001** (ì•½ê°„ì˜ íƒí—˜)
- **0.01** (ì ë‹¹í•œ íƒí—˜)
- **0.05** (ë§ì€ íƒí—˜)

**ì˜ˆìƒ íš¨ê³¼:**
- 0.0 â†’ ë¹ ë¥¸ ìˆ˜ë ´, ì§€ì—­ ìµœì í™” ìœ„í—˜
- ë†’ì€ ê°’ â†’ ë” ë§ì€ íƒí—˜, ëŠë¦° ìˆ˜ë ´, ë” ë‚˜ì€ ìµœì¢… ì„±ëŠ¥ ê°€ëŠ¥

**ì½”ë“œ:**
```python
config.training(entropy_coeff=0.01)
```

**ìŠ¤ì¼€ì¤„ ì˜µì…˜:**
```python
config.training(
    entropy_coeff=[[0, 0.01], [100000, 0.001], [500000, 0.0]]
)  # ì‹œê°„ì— ë”°ë¼ ê°ì†Œ
```

---

### 4. **KL Divergence ì„¤ì •** (`use_kl_loss`, `kl_coeff`, `kl_target`)
**ê¸°ë³¸ê°’:** `use_kl_loss=True`, `kl_coeff=0.2`, `kl_target=0.01`  
**ê³µì‹ ë¬¸ì„œ:** ì •ì±… ë³€í™”ë¥¼ ì œí•œí•˜ëŠ” KL divergence í˜ë„í‹°

**ì œì•ˆ ì‹¤í—˜ê°’:**

**A. KL Loss ì‚¬ìš© ì—¬ë¶€:**
- **True** (ê¸°ë³¸ê°’): KL í˜ë„í‹° ì‚¬ìš©
- **False**: KL í˜ë„í‹° ë¯¸ì‚¬ìš© (PPO í´ë¦¬í•‘ë§Œ)

**B. KL Coefficient:**
- **0.1** (ë‚®ìŒ)
- **0.2** (ê¸°ë³¸ê°’)
- **0.5** (ë†’ìŒ)

**C. KL Target:**
- **0.005** (ì—„ê²©)
- **0.01** (ê¸°ë³¸ê°’)
- **0.02** (ëŠìŠ¨)

**ì˜ˆìƒ íš¨ê³¼:**
- use_kl_loss=False â†’ í´ë¦¬í•‘ë§Œ ì‚¬ìš©, ë‹¨ìˆœí™”
- ë†’ì€ kl_coeff â†’ ì •ì±… ë³€í™” ê°•í•˜ê²Œ ì œí•œ
- ë‚®ì€ kl_target â†’ ë³´ìˆ˜ì  ì—…ë°ì´íŠ¸

**ì½”ë“œ:**
```python
config.training(
    use_kl_loss=True,
    kl_coeff=0.3,
    kl_target=0.01
)
```

---

### 5. **Gradient Clipping** (`grad_clip`)
**ê¸°ë³¸ê°’:** None  
**ê³µì‹ ë¬¸ì„œ:** ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ í´ë¦¬í•‘

**ì œì•ˆ ì‹¤í—˜ê°’:**
- **None** (í´ë¦¬í•‘ ì—†ìŒ)
- **0.5** (ê°•í•œ í´ë¦¬í•‘)
- **1.0** (ì¤‘ê°„ í´ë¦¬í•‘)
- **5.0** (ì•½í•œ í´ë¦¬í•‘)

**ì˜ˆìƒ íš¨ê³¼:**
- None â†’ í° ê·¸ë˜ë””ì–¸íŠ¸ í—ˆìš©, ë¶ˆì•ˆì • ê°€ëŠ¥
- ì‘ì€ ê°’ â†’ ì•ˆì •ì  í•™ìŠµ, ëŠë¦° ìˆ˜ë ´
- í° ê°’ â†’ ëŒ€ë¶€ë¶„ì˜ ê·¸ë˜ë””ì–¸íŠ¸ í†µê³¼

**ì½”ë“œ:**
```python
config.training(grad_clip=0.5)
```

---

### 6. **GAE ì‚¬ìš© ì—¬ë¶€** (`use_gae`, `use_critic`)
**ê¸°ë³¸ê°’:** `use_gae=True`, `use_critic=True`  
**ê³µì‹ ë¬¸ì„œ:** Generalized Advantage Estimation ì‚¬ìš©

**ì œì•ˆ ì‹¤í—˜ê°’:**
- **use_gae=True** (ê¸°ë³¸ê°’)
- **use_gae=False**: ë‹¨ìˆœ advantage ê³„ì‚°

**ì˜ˆìƒ íš¨ê³¼:**
- True â†’ bias-variance íŠ¸ë ˆì´ë“œì˜¤í”„ ì¡°ì ˆ
- False â†’ ë‹¨ìˆœí•˜ì§€ë§Œ ë†’ì€ ë¶„ì‚°

**ì½”ë“œ:**
```python
config.training(use_gae=False)  # ì‹¤í—˜ì 
```

---

### 7. **Discount Factor** (`gamma`)
**ê¸°ë³¸ê°’:** 0.99  
**ê³µì‹ ë¬¸ì„œ:** ë¯¸ë˜ ë³´ìƒ í• ì¸ìœ¨

**ì œì•ˆ ì‹¤í—˜ê°’:**
- **0.95** (ë‹¨ê¸° ë³´ìƒ ì¤‘ì‹œ)
- **0.99** (ê¸°ë³¸ê°’)
- **0.995** (ì¥ê¸° ë³´ìƒ ì¤‘ì‹œ)

**ì˜ˆìƒ íš¨ê³¼:**
- ë‚®ì€ ê°’ â†’ ì¦‰ê°ì  ë³´ìƒ ì„ í˜¸
- ë†’ì€ ê°’ â†’ ì¥ê¸°ì  ì „ëµ ì„ í˜¸

**ì½”ë“œ:**
```python
config.training(gamma=0.995)
```

---

### 8. **Learning Rate Schedule** (`lr_schedule`)
**ê¸°ë³¸ê°’:** None (ê³ ì • LR)  
**ê³µì‹ ë¬¸ì„œ:** í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§

**ì œì•ˆ ì‹¤í—˜ê°’:**
```python
# ì„ í˜• ê°ì†Œ
lr_schedule = [
    [0, 0.0003],
    [500000, 0.00001]
]

# ë‹¨ê³„ì  ê°ì†Œ
lr_schedule = [
    [0, 0.0003],
    [100000, 0.0001],
    [300000, 0.00003]
]
```

**ì˜ˆìƒ íš¨ê³¼:**
- ì´ˆê¸° ë†’ì€ LR â†’ ë¹ ë¥¸ í•™ìŠµ
- í›„ê¸° ë‚®ì€ LR â†’ ì•ˆì •ì  ìˆ˜ë ´

**ì½”ë“œ:**
```python
config.training(lr_schedule=[[0, 0.0003], [500000, 0.00001]])
```

**ì£¼ì˜:** ê³ ì • íŒŒë¼ë¯¸í„°(`lr=0.0003`)ì™€ ì¶©ëŒ ê°€ëŠ¥ì„± í™•ì¸ í•„ìš”

---

### 9. **SGD Minibatch í¬ê¸°** (`sgd_minibatch_size`)
**ê¸°ë³¸ê°’:** `minibatch_size`ì™€ ë™ì¼  
**ê³µì‹ ë¬¸ì„œ:** SGD ì—…ë°ì´íŠ¸ ì‹œ ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°

**ì°¸ê³ :** `minibatch_size`ëŠ” ê³ ì •ì´ì§€ë§Œ, ë‹¤ë¥¸ ë°°ì¹˜ ê´€ë ¨ ì„¤ì • í™•ì¸

---

### 10. **Rollout Fragment Length** (`rollout_fragment_length`)
**ê¸°ë³¸ê°’:** "auto"  
**ê³µì‹ ë¬¸ì„œ:** EnvRunnerê°€ ìˆ˜ì§‘í•˜ëŠ” íƒ€ì„ìŠ¤í… ìˆ˜

**ì œì•ˆ ì‹¤í—˜ê°’:**
- **200** (ì§§ìŒ)
- **400** (ì¤‘ê°„)
- **"auto"** (ê¸°ë³¸ê°’)

**ì˜ˆìƒ íš¨ê³¼:**
- ì§§ì€ ê¸¸ì´ â†’ ë¹ˆë²ˆí•œ ì—…ë°ì´íŠ¸
- ê¸´ ê¸¸ì´ â†’ íš¨ìœ¨ì  ìˆ˜ì§‘

**ì½”ë“œ:**
```python
config.env_runners(rollout_fragment_length=200)
```

---

### 11. **Exploration ì„¤ì •**
**ê¸°ë³¸ê°’:** ì—†ìŒ  
**ê³µì‹ ë¬¸ì„œ:** íƒí—˜ ì „ëµ ì¶”ê°€

**ì œì•ˆ ì‹¤í—˜:**
```python
config.exploration(
    explore=True,
    exploration_config={
        "type": "StochasticSampling",  # ê¸°ë³¸ê°’
    }
)
```

---

### 12. **Optimizer ì„¤ì •** (`_optimizer_config`)
**ê¸°ë³¸ê°’:** Adam optimizer  
**ê³µì‹ ë¬¸ì„œ:** ì˜µí‹°ë§ˆì´ì € ê´€ë ¨ ì„¤ì •

**ì œì•ˆ ì‹¤í—˜:**
```python
# Adam epsilon ì¡°ì •
config.training(
    _optimizer_config={
        "adam_epsilon": 1e-5  # ê¸°ë³¸ê°’ 1e-8
    }
)
```

---

## ğŸ§ª ì¶”ì²œ ì‹¤í—˜ ì¡°í•©

### ì‹¤í—˜ 1: í´ë¦¬í•‘ íŒŒë¼ë¯¸í„° ì˜í–¥
```python
experiments = [
    {"clip_param": 0.1, "vf_clip_param": 1.0},    # ë³´ìˆ˜ì 
    {"clip_param": 0.2, "vf_clip_param": 10.0},   # ê¸°ë³¸
    {"clip_param": 0.3, "vf_clip_param": 100.0},  # ê³µê²©ì 
]
```

### ì‹¤í—˜ 2: íƒí—˜ vs í™œìš©
```python
experiments = [
    {"entropy_coeff": 0.0},     # í™œìš© ì¤‘ì‹¬
    {"entropy_coeff": 0.01},    # ê· í˜•
    {"entropy_coeff": 0.05},    # íƒí—˜ ì¤‘ì‹¬
]
```

### ì‹¤í—˜ 3: ì•ˆì •ì„± ê°•í™”
```python
experiments = [
    {"grad_clip": None},                           # ì œí•œ ì—†ìŒ
    {"grad_clip": 0.5, "clip_param": 0.1},        # ê°•í•œ ì•ˆì •í™”
    {"grad_clip": 1.0, "use_kl_loss": True},      # ì¤‘ê°„ ì•ˆì •í™”
]
```

### ì‹¤í—˜ 4: KL Divergence íš¨ê³¼
```python
experiments = [
    {"use_kl_loss": False},                                    # KL ë¯¸ì‚¬ìš©
    {"use_kl_loss": True, "kl_coeff": 0.1, "kl_target": 0.01}, # ì•½í•œ KL
    {"use_kl_loss": True, "kl_coeff": 0.5, "kl_target": 0.005}, # ê°•í•œ KL
]
```

### ì‹¤í—˜ 5: ì¢…í•© ìµœì í™”
```python
experiments = [
    # ë¹ ë¥¸ ìˆ˜ë ´
    {
        "clip_param": 0.3,
        "entropy_coeff": 0.0,
        "grad_clip": None,
        "gamma": 0.95
    },
    # ì•ˆì •ì  í•™ìŠµ
    {
        "clip_param": 0.1,
        "entropy_coeff": 0.001,
        "grad_clip": 0.5,
        "use_kl_loss": True,
        "kl_coeff": 0.3
    },
    # íƒí—˜ ì¤‘ì‹¬
    {
        "clip_param": 0.2,
        "entropy_coeff": 0.05,
        "grad_clip": 1.0,
        "gamma": 0.995
    }
]
```

---

## ğŸ“Š ì¸¡ì • ì§€í‘œ (ë™ì¼)

### ì„±ëŠ¥
- `episode_reward_mean`: í‰ê·  ë³´ìƒ
- `episode_reward_std`: ë³´ìƒ í‘œì¤€í¸ì°¨ (5íšŒ trial)

### ì•ˆì •ì„±
- `reward_cv`: ë³€ë™ê³„ìˆ˜ (std/mean)
- `min/max reward`: ë²”ìœ„

### íš¨ìœ¨ì„±
- `SPS`: Steps Per Second
- `time_per_experiment`: ì†Œìš” ì‹œê°„

---

## ğŸ¯ ì‹¤í—˜ ëª©í‘œ

1. **í´ë¦¬í•‘ íš¨ê³¼**: clip_paramê³¼ vf_clip_paramì´ ì„±ëŠ¥/ì•ˆì •ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
2. **íƒí—˜ íš¨ê³¼**: entropy_coeffê°€ ìµœì¢… ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
3. **ì•ˆì •í™” ê¸°ë²•**: grad_clip, KL lossê°€ í•™ìŠµ ì•ˆì •ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
4. **í• ì¸ìœ¨ ì˜í–¥**: gammaê°€ ì¥ê¸°/ë‹¨ê¸° ì „ëµì— ë¯¸ì¹˜ëŠ” ì˜í–¥
5. **ì¡°í•© íš¨ê³¼**: ì—¬ëŸ¬ íŒŒë¼ë¯¸í„°ì˜ ìƒí˜¸ì‘ìš©

---

## ğŸ’¡ êµ¬í˜„ íŒ

### 1. ë² ì´ìŠ¤ë¼ì¸ ìœ ì§€
```python
def get_baseline_config():
    return {
        # ê³ ì • íŒŒë¼ë¯¸í„° (12-22 line)
        'lambda_': 0.95,
        'lr': 0.0003,
        'num_epochs': 15,
        'train_batch_size': 16384,
        'minibatch_size': 4096,
        'vf_loss_coeff': 0.01,
        'fcnet_hiddens': [64, 64],
        'fcnet_activation': 'tanh',
        'vf_share_layers': False,
        
        # ë³€ê²½ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’)
        'clip_param': 0.2,
        'vf_clip_param': 10.0,
        'entropy_coeff': 0.0,
        'use_kl_loss': True,
        'kl_coeff': 0.2,
        'kl_target': 0.01,
        'grad_clip': None,
        'gamma': 0.99,
        'use_gae': True,
        'use_critic': True,
    }
```

### 2. Config ì ìš©
```python
config = (
    PPOConfig()
    .environment("HalfCheetah-v5")
    .training(
        # ê³ ì • íŒŒë¼ë¯¸í„°
        lambda_=params['lambda_'],
        lr=params['lr'],
        num_epochs=params['num_epochs'],
        train_batch_size=params['train_batch_size'],
        minibatch_size=params['minibatch_size'],
        vf_loss_coeff=params['vf_loss_coeff'],
        model={
            "fcnet_hiddens": params['fcnet_hiddens'],
            "fcnet_activation": params['fcnet_activation'],
            "vf_share_layers": params['vf_share_layers'],
        },
        
        # ì‹¤í—˜ íŒŒë¼ë¯¸í„°
        clip_param=params['clip_param'],
        vf_clip_param=params['vf_clip_param'],
        entropy_coeff=params['entropy_coeff'],
        use_kl_loss=params['use_kl_loss'],
        kl_coeff=params['kl_coeff'],
        kl_target=params['kl_target'],
        grad_clip=params['grad_clip'],
        gamma=params['gamma'],
        use_gae=params['use_gae'],
        use_critic=params['use_critic'],
    )
)
```

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [RLlib PPO Config](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#proximal-policy-optimization-ppo)
- [RLlib Training API](https://docs.ray.io/en/latest/rllib/rllib-training.html)

---

## âœ… ìµœì¢… ê¶Œì¥ ì‹¤í—˜ ë¦¬ìŠ¤íŠ¸

ì´ **25ê°œ** ì‹¤í—˜ (ë² ì´ìŠ¤ë¼ì¸ + 24ê°œ ë³€í˜•)

1. **Baseline** (ê¸°ë³¸ê°’)
2-4. **Clip Parameter** (0.1, 0.3, ì¡°í•©)
5-7. **VF Clip Parameter** (1.0, 100.0, None)
8-11. **Entropy Coefficient** (0.001, 0.01, 0.05, ìŠ¤ì¼€ì¤„)
12-14. **KL Loss** (ë¯¸ì‚¬ìš©, ì•½í•¨, ê°•í•¨)
15-17. **Gradient Clipping** (0.5, 1.0, 5.0)
18-19. **Gamma** (0.95, 0.995)
20-21. **GAE** (False, True with different lambda)
22-25. **ì¡°í•© ì‹¤í—˜** (ë¹ ë¥¸ìˆ˜ë ´, ì•ˆì •í•™ìŠµ, íƒí—˜ì¤‘ì‹¬, ê· í˜•)

ê° ì‹¤í—˜ **5íšŒ ë°˜ë³µ** â†’ ì´ **125íšŒ** í•™ìŠµ ì‹¤í–‰
ì˜ˆìƒ ì†Œìš” ì‹œê°„: **3-4ì‹œê°„** (í™˜ê²½ì— ë”°ë¼ ë‹¤ë¦„)

---

**Good Luck! ğŸš€**
