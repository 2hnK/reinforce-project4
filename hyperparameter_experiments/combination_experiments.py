"""
20227128 ê¹€ì§€í›ˆ

íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜ - ì‹ ë¢°ì„± ìˆëŠ” ì¡°í•© í…ŒìŠ¤íŠ¸

ë‹¨ì¼ íŒŒë¼ë¯¸í„° ì‹¤í—˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸í—Œ ê²€ì¦ëœ ì¡°í•© ì‹¤í—˜ ìˆ˜í–‰

ì°¸ê³  ë¬¸í—Œ:
    - PPO ì›ë…¼ë¬¸ (Schulman et al., 2017)
    - OpenAI Spinning Up
    - RLlib ê³µì‹ ë¬¸ì„œ
    - CleanRL MuJoCo ë²¤ì¹˜ë§ˆí¬

í™˜ê²½ ì„¤ì •:
    - num_env_runners=8 (ê¶Œì¥: ë¬¼ë¦¬ ì½”ì–´ì˜ 50-75%)
    - num_envs_per_env_runner=5 (RLlib ê¶Œì¥ 4-8 ë²”ìœ„)
    - ì´ 40ê°œ í™˜ê²½ ë™ì‹œ ì‹¤í–‰ (PPO ë…¼ë¬¸ 32-64 envs ê¶Œì¥)
    - GPU ì‚¬ìš© (í•™ìŠµ ê°€ì†í™”)
    - ê·¼ê±°: Schulman et al. 2017, CleanRL MuJoCo benchmark
    - ì˜ˆìƒ SPS: 25,000-35,000
    - ì˜ˆìƒ íš¨ìœ¨: 50-60%
"""

import json
import time
from datetime import datetime
from pathlib import Path
import ray
from ray.rllib.algorithms.ppo import PPOConfig
import numpy as np


def get_baseline_config():
    """ë² ì´ìŠ¤ë¼ì¸ ì„¤ì • (ê³ ì • íŒŒë¼ë¯¸í„°)"""
    return {
        # ê³ ì • í•™ìŠµ íŒŒë¼ë¯¸í„° (ë³€ê²½ ë¶ˆê°€)
        'lambda_': 0.95,
        'lr': 0.0003,
        'num_epochs': 15,
        'train_batch_size': 32 * 512,  # 16384
        'minibatch_size': 4096,
        'vf_loss_coeff': 0.01,
        'fcnet_hiddens': [64, 64],
        'fcnet_activation': 'tanh',
        'vf_share_layers': False,
        
        # ì‹¤í—˜ ëŒ€ìƒ íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’)
        'clip_param': 0.2,
        'vf_clip_param': 10.0,
        'entropy_coeff': 0.0,
        'use_kl_loss': True,
        'kl_coeff': 0.2,
        'kl_target': 0.01,
        'grad_clip': None,
        'gamma': 0.99,
        'use_gae': True,
        'use_critic': True
    }


def get_combination_experiments():
    """ì‹ ë¢°ì„± ìˆëŠ” íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜ ì„¤ì •
    
    ì´ 7ê°œ ì‹¤í—˜:
    - Baseline (1ê°œ)
    - ë¬¸í—Œ ê²€ì¦ëœ ì¡°í•© (6ê°œ)
    """
    baseline = get_baseline_config()
    experiments = []
    
    # ===== 0. Baseline =====
    experiments.append({
        'name': 'baseline',
        'description': 'Baseline configuration (all defaults)',
        'params': baseline.copy(),
        'category': 'baseline',
        'rationale': 'PPO paper + RLlib defaults'
    })
    
    # ===== 1. ë¹ ë¥¸ ìˆ˜ë ´ (Fast Convergence) =====
    config = baseline.copy()
    config.update({
        'clip_param': 0.2,
        'gamma': 0.99,
        'use_kl_loss': False,
        'entropy_coeff': 0.0,
        'grad_clip': 1.0,
        'vf_clip_param': 10.0
    })
    experiments.append({
        'name': 'fast_convergence',
        'description': 'Fast convergence: standard PPO-Clip without KL',
        'params': config,
        'category': 'speed',
        'rationale': 'OpenAI Spinning Up + CleanRL benchmark',
        'expected': 'Faster learning, moderate stability'
    })
    
    # ===== 2. ìµœê³  ì•ˆì •ì„± (Ultra Stable) =====
    config = baseline.copy()
    config.update({
        'clip_param': 0.1,
        'gamma': 0.99,
        'grad_clip': 0.5,
        'vf_clip_param': 10.0,
        'use_kl_loss': True,
        'kl_coeff': 0.2,
        'kl_target': 0.01,
        'entropy_coeff': 0.0
    })
    experiments.append({
        'name': 'ultra_stable',
        'description': 'Maximum stability: conservative clip + gradient clip + KL',
        'params': config,
        'category': 'stability',
        'rationale': 'RLlib best practices + Stable-Baselines3',
        'expected': 'Highest stability, slower convergence'
    })
    
    # ===== 3. ê· í˜• ì¡°í•© (Balanced) =====
    config = baseline.copy()
    config.update({
        'clip_param': 0.2,
        'gamma': 0.99,
        'grad_clip': 1.0,
        'entropy_coeff': 0.01,
        'vf_clip_param': 10.0,
        'use_kl_loss': False
    })
    experiments.append({
        'name': 'balanced',
        'description': 'Balanced: standard PPO + minimal entropy',
        'params': config,
        'category': 'balanced',
        'rationale': 'Most widely used combination',
        'expected': 'Good balance between speed and stability'
    })
    
    # ===== 4. íƒí—˜ ì¤‘ì‹¬ (Exploration Focused) =====
    config = baseline.copy()
    config.update({
        'clip_param': 0.2,
        'gamma': 0.99,
        'entropy_coeff': 0.05,
        'grad_clip': 1.0,
        'vf_clip_param': 10.0,
        'use_kl_loss': False
    })
    experiments.append({
        'name': 'exploration_focused',
        'description': 'High exploration: entropy_coeff=0.05',
        'params': config,
        'category': 'exploration',
        'rationale': 'Atari environments setting',
        'expected': 'More diverse solutions, slower initial learning'
    })
    
    # ===== 5. ê³µê²©ì  í•™ìŠµ (Aggressive) =====
    config = baseline.copy()
    config.update({
        'clip_param': 0.3,
        'gamma': 0.99,
        'grad_clip': 1.0,
        'entropy_coeff': 0.0,
        'vf_clip_param': 10.0,
        'use_kl_loss': False
    })
    experiments.append({
        'name': 'aggressive',
        'description': 'Aggressive: high clip_param=0.3',
        'params': config,
        'category': 'speed',
        'rationale': 'Some research papers use up to 0.3',
        'expected': 'Very fast convergence, potential instability'
    })
    
    # ===== 6. Adaptive KL ìµœëŒ€ í™œìš© =====
    config = baseline.copy()
    config.update({
        'clip_param': 0.2,
        'gamma': 0.99,
        'use_kl_loss': True,
        'kl_coeff': 0.2,
        'kl_target': 0.01,
        'grad_clip': 0.5,
        'vf_clip_param': 10.0,
        'entropy_coeff': 0.0
    })
    experiments.append({
        'name': 'adaptive_kl',
        'description': 'Adaptive KL: PPO-Lagrange style',
        'params': config,
        'category': 'stability',
        'rationale': 'PPO-Lagrange, Adaptive KL research',
        'expected': 'Automatic constraint tuning, stable learning'
    })
    
    return experiments


def print_system_info():
    """ì‹œìŠ¤í…œ í™˜ê²½ ì •ë³´ ì¶œë ¥"""
    import platform
    import psutil

    try:
        import torch  # type: ignore
        torch_available = True
    except ImportError:
        torch_available = False
        torch = None  # type: ignore
    
    print("="*80)
    print("íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜ - ì‹œìŠ¤í…œ í™˜ê²½ ì •ë³´")
    print("="*80)
    
    # ê¸°ë³¸ ì •ë³´
    print(f"\n[ì‹œìŠ¤í…œ ì •ë³´]")
    print(f"  ìš´ì˜ì²´ì œ: {platform.system()} {platform.release()}")
    print(f"  Python: {platform.python_version()}")
    print(f"  Ray: {ray.__version__}")
    
    # CPU ì •ë³´
    print(f"\n[CPU ì •ë³´]")
    print(f"  í”„ë¡œì„¸ì„œ: {platform.processor()}")
    print(f"  ë¬¼ë¦¬ ì½”ì–´: {psutil.cpu_count(logical=False)}ê°œ")
    print(f"  ë…¼ë¦¬ ì½”ì–´: {psutil.cpu_count(logical=True)}ê°œ")
    
    # ë©”ëª¨ë¦¬ ì •ë³´
    memory = psutil.virtual_memory()
    print(f"\n[ë©”ëª¨ë¦¬ ì •ë³´]")
    print(f"  ì´ ë©”ëª¨ë¦¬: {memory.total / (1024**3):.1f}GB")
    print(f"  ì‚¬ìš© ê°€ëŠ¥: {memory.available / (1024**3):.1f}GB")
    
    # GPU ì •ë³´
    print(f"\n[GPU ì •ë³´]")
    if torch_available and torch.cuda.is_available():
        print(f"  CUDA ì‚¬ìš© ê°€ëŠ¥: Yes")
        print(f"  CUDA ë²„ì „: {torch.version.cuda}")
        print(f"  GPU ê°œìˆ˜: {torch.cuda.device_count()}ê°œ")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            props = torch.cuda.get_device_properties(i)
            print(f"    ë©”ëª¨ë¦¬: {props.total_memory / (1024**3):.1f}GB")
    elif torch_available:
        print(f"  CUDA ì‚¬ìš© ê°€ëŠ¥: No (PyTorch ì„¤ì¹˜ë¨)")
    else:
        print(f"  CUDA ì‚¬ìš© ê°€ëŠ¥: PyTorch ë¯¸ì„¤ì¹˜")
    
    # ë³‘ë ¬í™” ì„¤ì •
    print(f"\n[ë³‘ë ¬í™” ì„¤ì •]")
    print(f"  num_env_runners: 8")
    print(f"  num_envs_per_env_runner: 5")
    print(f"  ì´ í™˜ê²½ ìˆ˜: 40")
    print(f"  num_learners: 1 (GPU ì‚¬ìš©)")
    print(f"  ê·¼ê±°: CleanRL ë²¤ì¹˜ë§ˆí¬ + PPO ë…¼ë¬¸ (32-64 envs ê¶Œì¥)")
    print(f"  ì˜ˆìƒ SPS: 25,000-35,000")
    print(f"  ì˜ˆìƒ íš¨ìœ¨: 50-60%")
    
    print("="*80)


def run_single_trial(config_dict, exp_name, trial_num, num_iterations=10, save_checkpoint=False):
    """ë‹¨ì¼ ì‹œí–‰ ì‹¤í–‰
    
    Args:
        config_dict: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        exp_name: ì‹¤í—˜ ì´ë¦„
        trial_num: ì‹œí–‰ ë²ˆí˜¸
        num_iterations: ë°˜ë³µ íšŸìˆ˜
        save_checkpoint: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì—¬ë¶€
    """
    # PPO ì•Œê³ ë¦¬ì¦˜ ì„¤ì •
    config = (
        PPOConfig()
        .environment("HalfCheetah-v5")
        .training(
            lambda_=config_dict['lambda_'],
            lr=config_dict['lr'],
            num_epochs=config_dict['num_epochs'],
            train_batch_size=config_dict['train_batch_size'],
            minibatch_size=config_dict['minibatch_size'],
            vf_loss_coeff=config_dict['vf_loss_coeff'],
            clip_param=config_dict['clip_param'],
            vf_clip_param=config_dict['vf_clip_param'],
            entropy_coeff=config_dict['entropy_coeff'],
            use_kl_loss=config_dict['use_kl_loss'],
            kl_coeff=config_dict['kl_coeff'],
            kl_target=config_dict['kl_target'],
            grad_clip=config_dict['grad_clip'],
            gamma=config_dict['gamma'],
            use_gae=config_dict['use_gae'],
            use_critic=config_dict['use_critic'],
            model={
                "fcnet_hiddens": config_dict['fcnet_hiddens'],
                "fcnet_activation": config_dict['fcnet_activation'],
                "vf_share_layers": config_dict['vf_share_layers'],
            },
        )
        .learners(num_learners=1, num_gpus_per_learner=1)  # GPU ì‚¬ìš©ìœ¼ë¡œ í•™ìŠµ ê°€ì†í™”
        .debugging(seed=20227128 + trial_num)
        .env_runners(
            num_env_runners=8,           # 8ê°œ í”„ë¡œì„¸ìŠ¤ (ê¶Œì¥: ë¬¼ë¦¬ ì½”ì–´ì˜ 50-75%)
            num_envs_per_env_runner=5,   # ëŸ¬ë„ˆë‹¹ 5ê°œ í™˜ê²½ (RLlib ê¶Œì¥ 4-8)
            num_cpus_per_env_runner=1,   # ëŸ¬ë„ˆë‹¹ 1 CPU
        )
        .evaluation(
            evaluation_num_env_runners=1,
            evaluation_interval=0,
            evaluation_duration=5
        )
    )
    
    # ì•Œê³ ë¦¬ì¦˜ ë¹Œë“œ
    algo = config.build()
    
    results = []
    start_time = time.time()
    checkpoint_path = None
    
    try:
        for iteration in range(num_iterations):
            iter_start = time.time()
            result = algo.train()
            iter_time = time.time() - iter_start
            
            # ë©”íŠ¸ë¦­ ì¶”ì¶œ
            env_runners = result.get('env_runners', {})
            episode_reward_mean = env_runners.get('episode_reward_mean', 
                                                  env_runners.get('episode_return_mean', 0))
            
            # í•™ìŠµ ë‹¨ê³„ ì¶”ì¶œ (ì¤‘ìš”!)
            def _first_positive(*values):
                for val in values:
                    if isinstance(val, (int, float)) and val > 0:
                        return float(val)
                return 0.0

            num_env_steps_trained = _first_positive(
                result.get('num_env_steps_trained'),
                result.get('num_env_steps_trained_this_iter'),
                result.get('num_env_steps_trained_total'),
                result.get('counters', {}).get('num_env_steps_trained'),
                env_runners.get('num_env_steps_trained'),
                env_runners.get('num_env_steps_sampled'),
                result.get('num_env_steps_sampled')
            )
            
            # ì²« iterationì—ì„œ í•™ìŠµ ê²€ì¦
            if iteration == 0:
                if num_env_steps_trained == 0:
                    raise RuntimeError(
                        f"âŒ í•™ìŠµì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! "
                        f"num_env_steps_trained=0\n"
                        f"learner ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš” (num_learners >= 1 í•„ìš”)"
                    )
                else:
                    print(f"    âœ“ í•™ìŠµ ì‹œì‘ í™•ì¸: {num_env_steps_trained} steps trained")
            
            # SPS ê³„ì‚°
            num_env_steps = 0
            for key in ['num_env_steps_sampled', 'num_env_steps_sampled_this_iter']:
                if key in env_runners:
                    num_env_steps = env_runners[key]
                    break
            if num_env_steps == 0:
                num_env_steps = config_dict['train_batch_size']
            
            sps = num_env_steps / iter_time if iter_time > 0 else 0
            
            metrics = {
                'iteration': iteration + 1,
                'episode_reward_mean': float(episode_reward_mean),
                'episode_reward_min': float(env_runners.get('episode_reward_min', 0)),
                'episode_reward_max': float(env_runners.get('episode_reward_max', 0)),
                'episode_len_mean': float(env_runners.get('episode_len_mean', 0)),
                'num_env_steps_sampled': int(num_env_steps),
                'num_env_steps_trained': int(num_env_steps_trained),
                'time_this_iter_s': float(iter_time),
                'sps': float(sps),
            }
            
            results.append(metrics)
            
            print(f"    Iter {iteration + 1}/{num_iterations}: "
                  f"Reward={metrics['episode_reward_mean']:.2f}, "
                  f"Trained={num_env_steps_trained}, "
                  f"Time={iter_time:.2f}s, "
                  f"SPS={sps:.0f}")
    
        
        # ë§ˆì§€ë§‰ iteration ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì˜µì…˜)
        if save_checkpoint and iteration == num_iterations - 1:
            checkpoint_path = algo.save()
            print(f"    ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    finally:
        algo.stop()
    
    total_time = time.time() - start_time
    
    return {
        'trial_num': trial_num,
        'iterations': results,
        'total_time': float(total_time),
        'final_reward': float(results[-1]['episode_reward_mean']) if results else 0.0,
        'checkpoint_path': checkpoint_path
    }


def run_experiment(exp_config, num_trials=5, num_iterations=10, save_checkpoints=False):
    """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ (ì—¬ëŸ¬ ì‹œí–‰)
    
    Args:
        exp_config: ì‹¤í—˜ ì„¤ì •
        num_trials: ì‹œí–‰ íšŸìˆ˜
        num_iterations: ë°˜ë³µ íšŸìˆ˜
        save_checkpoints: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì—¬ë¶€ (ë§ˆì§€ë§‰ trialë§Œ)
    """
    exp_name = exp_config['name']
    print(f"\n{'='*80}")
    print(f"ì‹¤í—˜: {exp_name}")
    print(f"ì„¤ëª…: {exp_config['description']}")
    print(f"ì¹´í…Œê³ ë¦¬: {exp_config['category']}")
    print(f"ê·¼ê±°: {exp_config['rationale']}")
    if 'expected' in exp_config:
        print(f"ì˜ˆìƒ ê²°ê³¼: {exp_config['expected']}")
    print(f"{'='*80}")
    
    trials_results = []
    
    for trial in range(num_trials):
        print(f"\n  Trial {trial + 1}/{num_trials}")
        print(f"  {'='*50}")
        
        try:
            # ë§ˆì§€ë§‰ trialë§Œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_this_trial = save_checkpoints and (trial == num_trials - 1)
            trial_result = run_single_trial(
                exp_config['params'],
                exp_name,
                trial,
                num_iterations,
                save_checkpoint=save_this_trial
            )
            trials_results.append(trial_result)
            
        except Exception as e:
            print(f"  [ERROR] Trial {trial + 1} failed: {str(e)}")
            continue
    
    if not trials_results:
        print(f"  [WARNING] No successful trials for {exp_name}")
        return None
    
    # í†µê³„ ê³„ì‚°
    final_rewards = [t['final_reward'] for t in trials_results]
    all_iterations = [t['iterations'] for t in trials_results]
    
    # ê° iterationë³„ í‰ê·  ê³„ì‚°
    mean_rewards_per_iter = []
    if all_iterations:
        num_iters = len(all_iterations[0])
        for i in range(num_iters):
            iter_rewards = [trial[i]['episode_reward_mean'] for trial in all_iterations if len(trial) > i]
            if iter_rewards:
                mean_rewards_per_iter.append(np.mean(iter_rewards))
    
    # SPS í†µê³„
    all_sps = []
    for trial in trials_results:
        for iteration in trial['iterations']:
            if 'sps' in iteration and iteration['sps'] > 0:
                all_sps.append(iteration['sps'])
    
    statistics = {
        'final_reward_mean': float(np.mean(final_rewards)),
        'final_reward_std': float(np.std(final_rewards)),
        'final_reward_min': float(np.min(final_rewards)),
        'final_reward_max': float(np.max(final_rewards)),
        'final_reward_cv': float(np.std(final_rewards) / abs(np.mean(final_rewards))) if np.mean(final_rewards) != 0 else 0.0,
        'mean_rewards_per_iter': [float(x) for x in mean_rewards_per_iter],
        'sps_mean': float(np.mean(all_sps)) if all_sps else 0.0,
        'sps_std': float(np.std(all_sps)) if all_sps else 0.0,
    }
    
    print(f"\n  {'='*50}")
    print(f"  ìµœì¢… í†µê³„:")
    print(f"    í‰ê·  ë³´ìƒ: {statistics['final_reward_mean']:.2f} Â± {statistics['final_reward_std']:.2f}")
    print(f"    ë³€ë™ê³„ìˆ˜(CV): {statistics['final_reward_cv']:.4f}")
    print(f"    ë²”ìœ„: [{statistics['final_reward_min']:.2f}, {statistics['final_reward_max']:.2f}]")
    print(f"    í‰ê·  SPS: {statistics['sps_mean']:.0f} Â± {statistics['sps_std']:.0f}")
    
    return {
        'name': exp_name,
        'description': exp_config['description'],
        'category': exp_config['category'],
        'rationale': exp_config['rationale'],
        'expected': exp_config.get('expected', ''),
        'params': exp_config['params'],
        'trials': trials_results,
        'statistics': statistics
    }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*80)
    print("íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜ ì‹œì‘")
    print("20227128 ê¹€ì§€í›ˆ")
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()
    
    # ì‹¤í—˜ ì„¤ì • ë¡œë“œ
    experiments = get_combination_experiments()
    
    print(f"\nì´ {len(experiments)}ê°œì˜ ì¡°í•© ì‹¤í—˜ ì˜ˆì •")
    print(f"ê° ì‹¤í—˜ë‹¹ 5íšŒ ì‹œí–‰, ì‹œí–‰ë‹¹ 10íšŒ ë°˜ë³µ")
    print(f"ë³‘ë ¬í™”: 8 runners Ã— 5 envs = 40ê°œ í™˜ê²½")
    print(f"í•™ìŠµ: num_learners=1 (GPU ì‚¬ìš©)")
    print(f"ê·¼ê±°: PPO ë…¼ë¬¸ 32-64 envs, CleanRL 8 workers")
    print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 20-25ë¶„\n")
    
    # ì‚¬ìš©ì í™•ì¸
    response = input("ì‹¤í—˜ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): ")
    if response.lower() != 'yes':
        print("ì‹¤í—˜ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
        return
    
    # Ray ì´ˆê¸°í™”
    if not ray.is_initialized():
        ray.init(ignore_reinit_error=True)
    
    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    results_dir = Path(__file__).parent / "results"
    results_dir.mkdir(exist_ok=True)
    
    # ì‹¤í—˜ ì‹¤í–‰
    all_results = {
        'metadata': {
            'start_time': datetime.now().isoformat(),
            'num_experiments': len(experiments),
            'num_trials_per_experiment': 5,
            'num_iterations_per_trial': 10,
            'parallelization': {
                'num_env_runners': 8,
                'num_envs_per_env_runner': 5,
                'total_envs': 40,
                'num_learners': 1,
                'num_gpus_per_learner': 1,
                'rationale': 'PPO paper: 32-64 parallel envs, CleanRL: 8 workers'
            }
        },
        'experiments': []
    }
    
    start_time = time.time()
    
    for i, exp_config in enumerate(experiments):
        print(f"\n{'#'*80}")
        print(f"ì§„í–‰ ìƒí™©: ì‹¤í—˜ {i + 1}/{len(experiments)}")
        print(f"{'#'*80}")
        
        # baseline ì‹¤í—˜ë§Œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        save_ckpt = (exp_config['name'] == 'baseline')
        exp_result = run_experiment(exp_config, num_trials=5, num_iterations=10, 
                                   save_checkpoints=save_ckpt)
        
        if exp_result:
            all_results['experiments'].append(exp_result)
        
        # ì¤‘ê°„ ì €ì¥
        with open(results_dir / "combination_experiments_progress.json", 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        print(f"\n[ì§„í–‰ ì €ì¥: {results_dir}/combination_experiments_progress.json]")
    
    total_time = time.time() - start_time
    all_results['metadata']['end_time'] = datetime.now().isoformat()
    all_results['metadata']['total_time_seconds'] = float(total_time)
    all_results['metadata']['total_time_minutes'] = float(total_time / 60)
    
    # ìµœì¢… ì €ì¥
    with open(results_dir / "combination_experiments_final.json", 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print("\n" + "="*80)
    print("ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")
    print(f"ì´ ì†Œìš” ì‹œê°„: {total_time / 60:.1f}ë¶„ ({total_time / 3600:.2f}ì‹œê°„)")
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {results_dir}")
    print(f"  - ìµœì¢… ê²°ê³¼: combination_experiments_final.json")
    print(f"  - ì§„í–‰ ê¸°ë¡: combination_experiments_progress.json")
    print("="*80)
    
    # ê°„ë‹¨í•œ ìš”ì•½
    print("\nì‹¤í—˜ ìš”ì•½:")
    print(f"{'='*80}")
    print(f"{'ì‹¤í—˜ëª…':<25} {'ìµœì¢… ë³´ìƒ':<20} {'CV':<10} {'SPS':<10}")
    print(f"{'-'*80}")
    
    for exp in all_results['experiments']:
        stats = exp['statistics']
        print(f"{exp['name']:<25} "
              f"{stats['final_reward_mean']:>7.2f} Â± {stats['final_reward_std']:<7.2f} "
              f"{stats['final_reward_cv']:>8.4f} "
              f"{stats['sps_mean']:>9.0f}")
    
    print(f"{'='*80}")
    
    # Ray ì¢…ë£Œ
    ray.shutdown()


if __name__ == "__main__":
    main()
