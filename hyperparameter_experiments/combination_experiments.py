"""
20227128 ê¹€ì§€í›ˆ

íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜ - ì‹ ë¢°ì„± ìˆëŠ” ì¡°í•© í…ŒìŠ¤íŠ¸

ë‹¨ì¼ íŒŒë¼ë¯¸í„° ì‹¤í—˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸í—Œ ê²€ì¦ëœ ì¡°í•© ì‹¤í—˜ ìˆ˜í–‰

ì°¸ê³  ë¬¸í—Œ:
    - PPO ì›ë…¼ë¬¸ (Schulman et al., 2017)
    - OpenAI Spinning Up
    - RLlib ê³µì‹ ë¬¸ì„œ
    - CleanRL MuJoCo ë²¤ì¹˜ë§ˆí¬

í™˜ê²½ ì„¤ì •:
    - ê¸°ë³¸ num_env_runners=10, ê· í˜• ì‹¤í—˜ì€ 16ê¹Œì§€ í™•ì¥
    - num_envs_per_env_runner=5 (RLlib ê¶Œì¥ 4-8 ë²”ìœ„)
    - ì´ 50~80ê°œ í™˜ê²½ ë™ì‹œ ì‹¤í–‰ (PPO ë…¼ë¬¸ 32-64 envs ê¶Œì¥ ìƒë‹¨ ëŒ€ë¹„ ì—¬ìœ ë¶„ í™•ë³´)
    - GPU ì‚¬ìš© (í•™ìŠµ ê°€ì†í™”)
    - ê·¼ê±°: Schulman et al. 2017, CleanRL MuJoCo benchmark
    - ì˜ˆìƒ SPS: 25,000-35,000
    - ì˜ˆìƒ íš¨ìœ¨: 50-60%
"""

import json
import time
from datetime import datetime
from pathlib import Path
import ray
from ray.rllib.algorithms.ppo import PPOConfig
import numpy as np


def get_baseline_config():
    """ë² ì´ìŠ¤ë¼ì¸ ì„¤ì • (ê³ ì • íŒŒë¼ë¯¸í„°)"""
    return {
        # ê³ ì • í•™ìŠµ íŒŒë¼ë¯¸í„° (ë³€ê²½ ë¶ˆê°€)
        'lambda_': 0.95,
        'lr': 0.0003,
        'num_epochs': 15,
        'train_batch_size': 32 * 512,  # 16384
        'minibatch_size': 4096,
        'vf_loss_coeff': 0.01,
        'fcnet_hiddens': [64, 64],
        'fcnet_activation': 'tanh',
        'vf_share_layers': False,
        
        # ì‹¤í—˜ ëŒ€ìƒ íŒŒë¼ë¯¸í„° (ê¸°ë³¸ê°’)
        'clip_param': 0.2,
        'vf_clip_param': 10.0,
        'entropy_coeff': 0.0,
        'use_kl_loss': True,
        'kl_coeff': 0.2,
        'kl_target': 0.01,
        'grad_clip': None,
        'gamma': 0.99,
        'use_gae': True,
        'use_critic': True
    }


def get_combination_experiments():
    """5ê°œ ì‹¤í—˜ í™˜ê²½ ì„¸íŠ¸"""
    baseline = get_baseline_config()

    def with_common_overrides(**overrides):
        cfg = baseline.copy()
        cfg.update(overrides)
        return cfg

    return [
        {
            'name': 'baseline_conservative',
            'description': 'Baseline (conservative defaults)',
            'category': 'baseline',
            'rationale': 'More conservative than baseline_default for variance reduction',
            'expected': 'Similar or slightly lower reward with lower CV',
            'params': with_common_overrides(
                clip_param=0.12,
                gamma=0.99,
                kl_target=0.005,
                kl_coeff=0.3,
                entropy_coeff=0.0,
                optimizer_config={'sgd_momentum': 0.0},
                model_overrides={'action_dist_config': {'initial_log_std': -1.0}},
            ),
            'runner_overrides': {'num_env_runners': 16}
        },
        {
            'name': 'aggressive_exploration',
            'description': 'Aggressive exploration with scheduled entropy/clip',
            'category': 'speed',
            'rationale': 'Fast initial learning via high clip & entropy',
            'expected': 'Fast early gains, late instability risk',
            'params': with_common_overrides(
                clip_param=0.3,
                gamma=0.95,
                kl_target=0.05,
                entropy_coeff=0.01,
            ),
            'runner_overrides': {'num_env_runners': 16}
        },
        {
            'name': 'stable_conservative',
            'description': 'Stable conservative setting with weight decay',
            'category': 'stability',
            'rationale': 'Maximum stability focus',
            'expected': 'Slowest but most stable convergence',
            'params': with_common_overrides(
                clip_param=0.1,
                gamma=0.99,
                kl_target=0.01,
                entropy_coeff=0.0,
                optimizer_config={'sgd_momentum': 0.99},
                model_overrides={'action_dist_config': {'initial_log_std': -1.2}}
            ),
            'runner_overrides': {'num_env_runners': 16}
        },
        {
            'name': 'balanced_high_momentum',
            'description': 'Balanced config with high momentum and more workers',
            'category': 'balanced',
            'rationale': 'Practical balance of speed and stability',
            'expected': 'Fast convergence while remaining stable',
            'params': with_common_overrides(
                clip_param=0.25,
                gamma=0.99,
                kl_target=0.02,
                entropy_coeff=0.0,
                entropy_coeff_schedule=[
                    [0,    0.01],
                    [200_000, 0.005],
                    [500_000, 0.0],
                ],
                optimizer_config={'sgd_momentum': 0.9},
                model_overrides={'action_dist_config': {'initial_log_std': -0.7}}
            ),
            'runner_overrides': {'num_env_runners': 16}
        },
        {
            'name': 'kl_focused',
            'description': 'KL-focused training with stronger penalties',
            'category': 'stability',
            'rationale': 'Tight KL and VF regularisation',
            'expected': 'Most accurate value estimates',
            'params': with_common_overrides(
                clip_param=0.25,
                gamma=0.99,
                kl_target=0.01,
                kl_coeff=0.2,
                entropy_coeff=0.003,
                optimizer_config={'sgd_momentum': 0.9},
                vf_loss_coeff=0.5,
                model_overrides={'action_dist_config': {'initial_log_std': -0.8}}
            ),
            'runner_overrides': {'num_env_runners': 16}
        },
        {
            'name': 'performance_tuned_v1',
            'description': 'PPO config tuned for HalfCheetah performance',
            'category': 'performance',
            'rationale': 'Based on PPO paper, CleanRL, SB3 MuJoCo defaults',
            'expected': 'Higher final return than baseline_default',
            'params': with_common_overrides(
                clip_param=0.27,
                gamma=0.99,
                entropy_coeff=0.0,
                entropy_coeff_schedule=[
                    [0,      0.02],
                    [200_000, 0.01],
                    [500_000, 0.0],
                ],
                use_kl_loss=True,
                kl_target=0.02,
                kl_coeff=0.15,
                optimizer_config={'sgd_momentum': 0.9},
                model_overrides={'action_dist_config': {'initial_log_std': -0.7}},
            ),
            'runner_overrides': {'num_env_runners': 16}
        },
    ]


DEFAULT_RUNNER_CONFIG = {
    'num_env_runners': 16,
    'num_envs_per_env_runner': 5,
    'num_cpus_per_env_runner': 1,
}


def _scheduled_value(schedule, current_step):
    if not schedule:
        return None
    value = schedule[0][1]
    for step, val in schedule:
        if current_step >= step:
            value = val
        else:
            break
    return value


def _apply_clip_param(algo, new_value):
    if new_value is None:
        return
    algo.config['clip_param'] = new_value

    def _set(policy, *_):
        policy.config['clip_param'] = new_value

    algo.workers.foreach_policy(_set)


def print_system_info():
    """ì‹œìŠ¤í…œ í™˜ê²½ ì •ë³´ ì¶œë ¥"""
    import platform
    import psutil

    try:
        import torch  # type: ignore
        torch_available = True
    except ImportError:
        torch_available = False
        torch = None  # type: ignore
    
    print("="*80)
    print("íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜ - ì‹œìŠ¤í…œ í™˜ê²½ ì •ë³´")
    print("="*80)
    
    # ê¸°ë³¸ ì •ë³´
    print(f"\n[ì‹œìŠ¤í…œ ì •ë³´]")
    print(f"  ìš´ì˜ì²´ì œ: {platform.system()} {platform.release()}")
    print(f"  Python: {platform.python_version()}")
    print(f"  Ray: {ray.__version__}")
    
    # CPU ì •ë³´
    print(f"\n[CPU ì •ë³´]")
    print(f"  í”„ë¡œì„¸ì„œ: {platform.processor()}")
    print(f"  ë¬¼ë¦¬ ì½”ì–´: {psutil.cpu_count(logical=False)}ê°œ")
    print(f"  ë…¼ë¦¬ ì½”ì–´: {psutil.cpu_count(logical=True)}ê°œ")
    
    # ë©”ëª¨ë¦¬ ì •ë³´
    memory = psutil.virtual_memory()
    print(f"\n[ë©”ëª¨ë¦¬ ì •ë³´]")
    print(f"  ì´ ë©”ëª¨ë¦¬: {memory.total / (1024**3):.1f}GB")
    print(f"  ì‚¬ìš© ê°€ëŠ¥: {memory.available / (1024**3):.1f}GB")
    
    # GPU ì •ë³´
    print(f"\n[GPU ì •ë³´]")
    if torch_available and torch.cuda.is_available():
        print(f"  CUDA ì‚¬ìš© ê°€ëŠ¥: Yes")
        print(f"  CUDA ë²„ì „: {torch.version.cuda}")
        print(f"  GPU ê°œìˆ˜: {torch.cuda.device_count()}ê°œ")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            props = torch.cuda.get_device_properties(i)
            print(f"    ë©”ëª¨ë¦¬: {props.total_memory / (1024**3):.1f}GB")
    elif torch_available:
        print(f"  CUDA ì‚¬ìš© ê°€ëŠ¥: No (PyTorch ì„¤ì¹˜ë¨)")
    else:
        print(f"  CUDA ì‚¬ìš© ê°€ëŠ¥: PyTorch ë¯¸ì„¤ì¹˜")
    
    # ë³‘ë ¬í™” ì„¤ì •
    print(f"\n[ë³‘ë ¬í™” ê¸°ë³¸ ì„¤ì •]")
    print(f"  num_env_runners: {DEFAULT_RUNNER_CONFIG['num_env_runners']}")
    print(f"  num_envs_per_env_runner: {DEFAULT_RUNNER_CONFIG['num_envs_per_env_runner']}")
    total_envs = DEFAULT_RUNNER_CONFIG['num_env_runners'] * DEFAULT_RUNNER_CONFIG['num_envs_per_env_runner']
    print(f"  ì´ í™˜ê²½ ìˆ˜: {total_envs}")
    print(f"  num_learners: 1 (GPU ì‚¬ìš©)")
        print(f"  ê·¼ê±°: CleanRL ë²¤ì¹˜ë§ˆí¬ + PPO ë…¼ë¬¸ (32-64 envs ê¶Œì¥, ê¸°ë³¸ 10 runners)")
    print(f"  ì˜ˆìƒ SPS: 25,000-35,000")
    print(f"  ì˜ˆìƒ íš¨ìœ¨: 50-60%")
    
    print("="*80)


def run_single_trial(config_dict, exp_name, trial_num, num_iterations=10, save_checkpoint=False, runner_overrides=None):
    """ë‹¨ì¼ ì‹œí–‰ ì‹¤í–‰
    
    Args:
        config_dict: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        exp_name: ì‹¤í—˜ ì´ë¦„
        trial_num: ì‹œí–‰ ë²ˆí˜¸
        num_iterations: ë°˜ë³µ íšŸìˆ˜
        save_checkpoint: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì—¬ë¶€
    """
    runner_cfg = DEFAULT_RUNNER_CONFIG.copy()
    if runner_overrides:
        runner_cfg.update(runner_overrides)

    clip_schedule = config_dict.get('clip_param_schedule')
    initial_clip = _scheduled_value(clip_schedule, 0) or config_dict['clip_param']

    model_config = {
        "fcnet_hiddens": config_dict['fcnet_hiddens'],
        "fcnet_activation": config_dict['fcnet_activation'],
        "vf_share_layers": config_dict['vf_share_layers'],
    }
    if config_dict.get('model_overrides'):
        model_config.update(config_dict['model_overrides'])

    optimizer_config = config_dict.get('optimizer_config')

    training_kwargs = dict(
        lambda_=config_dict['lambda_'],
        lr=config_dict['lr'],
        num_epochs=config_dict['num_epochs'],
        train_batch_size=config_dict['train_batch_size'],
        minibatch_size=config_dict['minibatch_size'],
        vf_loss_coeff=config_dict['vf_loss_coeff'],
        clip_param=initial_clip,
        vf_clip_param=config_dict['vf_clip_param'],
        entropy_coeff=config_dict['entropy_coeff'],
        use_kl_loss=config_dict['use_kl_loss'],
        kl_coeff=config_dict['kl_coeff'],
        kl_target=config_dict['kl_target'],
        grad_clip=config_dict['grad_clip'],
        gamma=config_dict['gamma'],
        use_gae=config_dict['use_gae'],
        use_critic=config_dict['use_critic'],
        model=model_config,
    )

    if config_dict.get('entropy_coeff_schedule'):
        training_kwargs['entropy_coeff_schedule'] = config_dict['entropy_coeff_schedule']
    if optimizer_config:
        training_kwargs['optimizer_config'] = optimizer_config

    config = (
        PPOConfig()
        .environment("HalfCheetah-v5")
        .training(**training_kwargs)
        .learners(num_learners=1, num_gpus_per_learner=1)  # GPU ì‚¬ìš©ìœ¼ë¡œ í•™ìŠµ ê°€ì†í™”
        .debugging(seed=20227128 + trial_num)
        .env_runners(
            num_env_runners=runner_cfg['num_env_runners'],
            num_envs_per_env_runner=runner_cfg['num_envs_per_env_runner'],
            num_cpus_per_env_runner=runner_cfg['num_cpus_per_env_runner'],
        )
        .evaluation(
            evaluation_num_env_runners=1,
            evaluation_interval=0,
            evaluation_duration=5
        )
    )
    
    # ì•Œê³ ë¦¬ì¦˜ ë¹Œë“œ
    algo = config.build()
    
    results = []
    start_time = time.time()
    checkpoint_path = None

    _apply_clip_param(algo, initial_clip)
    total_env_steps = 0
    
    try:
        for iteration in range(num_iterations):
            scheduled_clip = _scheduled_value(clip_schedule, total_env_steps)
            current_clip = algo.config.get('clip_param', initial_clip)
            if scheduled_clip is not None and abs(scheduled_clip - current_clip) > 1e-6:
                _apply_clip_param(algo, scheduled_clip)
                print(f"    â†º clip_param schedule applied: {scheduled_clip:.3f} (steps={total_env_steps})")
            
            iter_start = time.time()
            result = algo.train()
            iter_time = time.time() - iter_start
            
            # ë©”íŠ¸ë¦­ ì¶”ì¶œ
            env_runners = result.get('env_runners', {})
            episode_reward_mean = env_runners.get('episode_reward_mean', 
                                                  env_runners.get('episode_return_mean', 0))
            
            # í•™ìŠµ ë‹¨ê³„ ì¶”ì¶œ (ì¤‘ìš”!)
            def _first_positive(*values):
                for val in values:
                    if isinstance(val, (int, float)) and val > 0:
                        return float(val)
                return 0.0

            num_env_steps_trained = _first_positive(
                result.get('num_env_steps_trained'),
                result.get('num_env_steps_trained_this_iter'),
                result.get('num_env_steps_trained_total'),
                result.get('counters', {}).get('num_env_steps_trained'),
                env_runners.get('num_env_steps_trained'),
                env_runners.get('num_env_steps_sampled'),
                result.get('num_env_steps_sampled')
            )
            
            # ì²« iterationì—ì„œ í•™ìŠµ ê²€ì¦
            if iteration == 0:
                if num_env_steps_trained == 0:
                    raise RuntimeError(
                        f"âŒ í•™ìŠµì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! "
                        f"num_env_steps_trained=0\n"
                        f"learner ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš” (num_learners >= 1 í•„ìš”)"
                    )
                else:
                    print(f"    âœ“ í•™ìŠµ ì‹œì‘ í™•ì¸: {num_env_steps_trained} steps trained")
            
            # SPS ê³„ì‚°
            num_env_steps = 0
            for key in ['num_env_steps_sampled', 'num_env_steps_sampled_this_iter']:
                if key in env_runners:
                    num_env_steps = env_runners[key]
                    break
            if num_env_steps == 0:
                num_env_steps = config_dict['train_batch_size']
            
            sps = num_env_steps / iter_time if iter_time > 0 else 0
            
            metrics = {
                'iteration': iteration + 1,
                'episode_reward_mean': float(episode_reward_mean),
                'episode_reward_min': float(env_runners.get('episode_reward_min', 0)),
                'episode_reward_max': float(env_runners.get('episode_reward_max', 0)),
                'episode_len_mean': float(env_runners.get('episode_len_mean', 0)),
                'num_env_steps_sampled': int(num_env_steps),
                'num_env_steps_trained': int(num_env_steps_trained),
                'time_this_iter_s': float(iter_time),
                'sps': float(sps),
            }
            
            results.append(metrics)
            total_env_steps = max(total_env_steps, int(result.get('num_env_steps_sampled', total_env_steps)))
            
            print(f"    Iter {iteration + 1}/{num_iterations}: "
                  f"Reward={metrics['episode_reward_mean']:.2f}, "
                  f"Trained={num_env_steps_trained}, "
                  f"Time={iter_time:.2f}s, "
                  f"SPS={sps:.0f}")
    
        
        # ë§ˆì§€ë§‰ iteration ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì˜µì…˜)
        if save_checkpoint and iteration == num_iterations - 1:
            raw_checkpoint = algo.save()
            if isinstance(raw_checkpoint, (str, Path)):
                checkpoint_path = str(raw_checkpoint)
            elif hasattr(raw_checkpoint, "to_uri"):
                checkpoint_path = raw_checkpoint.to_uri()
            elif hasattr(raw_checkpoint, "path"):
                checkpoint_path = str(raw_checkpoint.path)
            else:
                checkpoint_path = str(raw_checkpoint)
            print(f"    ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    finally:
        algo.stop()
    
    total_time = time.time() - start_time
    
    return {
        'trial_num': trial_num,
        'iterations': results,
        'total_time': float(total_time),
        'final_reward': float(results[-1]['episode_reward_mean']) if results else 0.0,
        'checkpoint_path': checkpoint_path
    }


def run_experiment(exp_config, num_trials=5, num_iterations=10, save_checkpoints=False):
    """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ (ì—¬ëŸ¬ ì‹œí–‰)
    
    Args:
        exp_config: ì‹¤í—˜ ì„¤ì •
        num_trials: ì‹œí–‰ íšŸìˆ˜
        num_iterations: ë°˜ë³µ íšŸìˆ˜
        save_checkpoints: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì—¬ë¶€ (ë§ˆì§€ë§‰ trialë§Œ)
    """
    exp_name = exp_config['name']
    print(f"\n{'='*80}")
    print(f"ì‹¤í—˜: {exp_name}")
    print(f"ì„¤ëª…: {exp_config['description']}")
    print(f"ì¹´í…Œê³ ë¦¬: {exp_config['category']}")
    print(f"ê·¼ê±°: {exp_config['rationale']}")
    if 'expected' in exp_config:
        print(f"ì˜ˆìƒ ê²°ê³¼: {exp_config['expected']}")
    print(f"{'='*80}")
    
    trials_results = []
    runner_overrides = exp_config.get('runner_overrides')
    
    for trial in range(num_trials):
        print(f"\n  Trial {trial + 1}/{num_trials}")
        print(f"  {'='*50}")
        
        try:
            # ë§ˆì§€ë§‰ trialë§Œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_this_trial = save_checkpoints and (trial == num_trials - 1)
            trial_result = run_single_trial(
                exp_config['params'],
                exp_name,
                trial,
                num_iterations,
                save_checkpoint=save_this_trial,
                runner_overrides=runner_overrides
            )
            trials_results.append(trial_result)
            
        except Exception as e:
            print(f"  [ERROR] Trial {trial + 1} failed: {str(e)}")
            continue
    
    if not trials_results:
        print(f"  [WARNING] No successful trials for {exp_name}")
        return None
    
    # í†µê³„ ê³„ì‚°
    final_rewards = [t['final_reward'] for t in trials_results]
    all_iterations = [t['iterations'] for t in trials_results]
    
    # ê° iterationë³„ í‰ê·  ê³„ì‚°
    mean_rewards_per_iter = []
    if all_iterations:
        num_iters = len(all_iterations[0])
        for i in range(num_iters):
            iter_rewards = [trial[i]['episode_reward_mean'] for trial in all_iterations if len(trial) > i]
            if iter_rewards:
                mean_rewards_per_iter.append(np.mean(iter_rewards))
    
    # SPS í†µê³„
    all_sps = []
    for trial in trials_results:
        for iteration in trial['iterations']:
            if 'sps' in iteration and iteration['sps'] > 0:
                all_sps.append(iteration['sps'])
    
    statistics = {
        'final_reward_mean': float(np.mean(final_rewards)),
        'final_reward_std': float(np.std(final_rewards)),
        'final_reward_min': float(np.min(final_rewards)),
        'final_reward_max': float(np.max(final_rewards)),
        'final_reward_cv': float(np.std(final_rewards) / abs(np.mean(final_rewards))) if np.mean(final_rewards) != 0 else 0.0,
        'mean_rewards_per_iter': [float(x) for x in mean_rewards_per_iter],
        'sps_mean': float(np.mean(all_sps)) if all_sps else 0.0,
        'sps_std': float(np.std(all_sps)) if all_sps else 0.0,
    }
    
    print(f"\n  {'='*50}")
    print(f"  ìµœì¢… í†µê³„:")
    print(f"    í‰ê·  ë³´ìƒ: {statistics['final_reward_mean']:.2f} Â± {statistics['final_reward_std']:.2f}")
    print(f"    ë³€ë™ê³„ìˆ˜(CV): {statistics['final_reward_cv']:.4f}")
    print(f"    ë²”ìœ„: [{statistics['final_reward_min']:.2f}, {statistics['final_reward_max']:.2f}]")
    print(f"    í‰ê·  SPS: {statistics['sps_mean']:.0f} Â± {statistics['sps_std']:.0f}")
    
    return {
        'name': exp_name,
        'description': exp_config['description'],
        'category': exp_config['category'],
        'rationale': exp_config['rationale'],
        'expected': exp_config.get('expected', ''),
        'params': exp_config['params'],
        'runner_overrides': exp_config.get('runner_overrides'),
        'trials': trials_results,
        'statistics': statistics
    }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*80)
    print("íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜ ì‹œì‘")
    print("20227128 ê¹€ì§€í›ˆ")
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()
    
    # ì‹¤í—˜ ì„¤ì • ë¡œë“œ
    experiments = get_combination_experiments()
    
    print(f"\nì´ {len(experiments)}ê°œì˜ ì¡°í•© ì‹¤í—˜ ì˜ˆì •")
    print(f"ê° ì‹¤í—˜ë‹¹ 5íšŒ ì‹œí–‰, ì‹œí–‰ë‹¹ 10íšŒ ë°˜ë³µ")
    total_envs = DEFAULT_RUNNER_CONFIG['num_env_runners'] * DEFAULT_RUNNER_CONFIG['num_envs_per_env_runner']
    print(f"ë³‘ë ¬í™”: ê¸°ë³¸ {DEFAULT_RUNNER_CONFIG['num_env_runners']} runners Ã— {DEFAULT_RUNNER_CONFIG['num_envs_per_env_runner']} envs = {total_envs}ê°œ í™˜ê²½ (ê· í˜• ì‹¤í—˜ì€ 16 runners)")
    print(f"í•™ìŠµ: num_learners=1 (GPU ì‚¬ìš©)")
    print(f"ê·¼ê±°: PPO ë…¼ë¬¸ 32-64 envs, CleanRL 8 workers â†’ ê¸°ë³¸ 10 runnersë¡œ í™•ì¥")
    print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 20-25ë¶„\n")
    
    # ì‚¬ìš©ì í™•ì¸
    response = input("ì‹¤í—˜ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): ")
    if response.lower() != 'yes':
        print("ì‹¤í—˜ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
        return
    
    # Ray ì´ˆê¸°í™”
    if not ray.is_initialized():
        ray.init(ignore_reinit_error=True)
    
    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    results_dir = Path(__file__).parent / "results"
    results_dir.mkdir(exist_ok=True)
    
    # ì‹¤í—˜ ì‹¤í–‰
    all_results = {
        'metadata': {
            'start_time': datetime.now().isoformat(),
            'num_experiments': len(experiments),
            'num_trials_per_experiment': 5,
            'num_iterations_per_trial': 10,
            'parallelization': {
                'default_num_env_runners': DEFAULT_RUNNER_CONFIG['num_env_runners'],
                'num_envs_per_env_runner': DEFAULT_RUNNER_CONFIG['num_envs_per_env_runner'],
                'total_envs_default': total_envs,
                'special_cases': {'balanced_high_momentum': 16},
                'num_learners': 1,
                'num_gpus_per_learner': 1,
                'rationale': 'PPO paper: 32-64 parallel envs, CleanRL: 8 workers (default expanded to 10 runners)'
            }
        },
        'experiments': []
    }
    
    start_time = time.time()
    
    for i, exp_config in enumerate(experiments):
        print(f"\n{'#'*80}")
        print(f"ì§„í–‰ ìƒí™©: ì‹¤í—˜ {i + 1}/{len(experiments)}")
        print(f"{'#'*80}")
        
        # baseline ì‹¤í—˜ë§Œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        save_ckpt = (exp_config['name'] == 'baseline_conservative')
        exp_result = run_experiment(exp_config, num_trials=5, num_iterations=10, 
                                   save_checkpoints=save_ckpt)
        
        if exp_result:
            all_results['experiments'].append(exp_result)
        
        # ì¤‘ê°„ ì €ì¥
        with open(results_dir / "combination_experiments_progress.json", 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        print(f"\n[ì§„í–‰ ì €ì¥: {results_dir}/combination_experiments_progress.json]")
    
    total_time = time.time() - start_time
    all_results['metadata']['end_time'] = datetime.now().isoformat()
    all_results['metadata']['total_time_seconds'] = float(total_time)
    all_results['metadata']['total_time_minutes'] = float(total_time / 60)
    
    # ìµœì¢… ì €ì¥
    with open(results_dir / "combination_experiments_final.json", 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print("\n" + "="*80)
    print("ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")
    print(f"ì´ ì†Œìš” ì‹œê°„: {total_time / 60:.1f}ë¶„ ({total_time / 3600:.2f}ì‹œê°„)")
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {results_dir}")
    print(f"  - ìµœì¢… ê²°ê³¼: combination_experiments_final.json")
    print(f"  - ì§„í–‰ ê¸°ë¡: combination_experiments_progress.json")
    print("="*80)
    
    # ê°„ë‹¨í•œ ìš”ì•½
    print("\nì‹¤í—˜ ìš”ì•½:")
    print(f"{'='*80}")
    print(f"{'ì‹¤í—˜ëª…':<25} {'ìµœì¢… ë³´ìƒ':<20} {'CV':<10} {'SPS':<10}")
    print(f"{'-'*80}")
    
    for exp in all_results['experiments']:
        stats = exp['statistics']
        print(f"{exp['name']:<25} "
            f"{stats['final_reward_mean']:>7.2f} Â± {stats['final_reward_std']:<7.2f} "
            f"{stats['final_reward_cv']:>8.4f} "
            f"{stats['sps_mean']:>9.0f}")
    
    print(f"{'='*80}")
    
    # Ray ì¢…ë£Œ
    ray.shutdown()


if __name__ == "__main__":
    main()
