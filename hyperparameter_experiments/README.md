# Hyperparameter Stability and Performance Experiments

**Student ID:** 20227128 κΉ€μ§€ν›

## π“‹ μ‹¤ν— κ°μ”

PPO μ•κ³ λ¦¬μ¦μ ν•μ΄νΌνλΌλ―Έν„° λ³€κ²½μ— λ”°λ¥Έ ν•™μµ μ„±λ¥ λ° μ•μ •μ„± λΉ„κµ μ‹¤ν—

### λ©ν‘
- **μ„±λ¥ λΉ„κµ**: κ° μ„¤μ •μ—μ„ λ‹¬μ„±ν• ν‰κ·  λ³΄μƒ
- **μ•μ •μ„± λΉ„κµ**: 5ν μ‹¤ν–‰μ ν‘μ¤€νΈμ°¨ λ° λ³€λ™κ³„μ(CV)
- **ν¨μ¨μ„± λΉ„κµ**: SPS(Steps Per Second), ν•™μµ μ‹κ°„

---

## π”§ λ² μ΄μ¤λΌμΈ μ„¤μ •

```python
lambda_ = 0.95              # GAE Lambda
lr = 0.0003                 # Learning Rate
num_epochs = 15             # Training Epochs
train_batch_size = 16384    # Batch Size (32 Γ— 512)
minibatch_size = 4096       # Minibatch Size
vf_loss_coeff = 0.01        # Value Function Loss Coefficient
fcnet_hiddens = [64, 64]    # Network Architecture
fcnet_activation = "tanh"   # Activation Function
clip_param = 0.2            # PPO Clip Parameter
entropy_coeff = 0.0         # Entropy Coefficient
```

---

## π§ μ‹¤ν— νλΌλ―Έν„°

### 1. Learning Rate (lr)
ν•™μµ μ†λ„λ¥Ό μ μ–΄ν•λ” νλΌλ―Έν„°
- **0.0001** (λ‚®μ): μ•μ •μ μ΄μ§€λ§ λλ¦° ν•™μµ
- **0.0003** (λ² μ΄μ¤λΌμΈ)
- **0.001** (λ†’μ): λΉ λ¥΄μ§€λ§ λ¶μ•μ •ν•  μ μμ

**μμƒ ν¨κ³Ό:**
- λ‚®μ€ LR β†’ μ•μ •μ , λλ¦° μλ ΄
- λ†’μ€ LR β†’ λΉ λ¥Έ μλ ΄, λ¶μ•μ • κ°€λ¥

---

### 2. GAE Lambda (Ξ»)
Advantage μ¶”μ •μ bias-variance νΈλ μ΄λ“μ¤ν”„
- **0.9** (λ‚®μ): Low bias, high variance
- **0.95** (λ² μ΄μ¤λΌμΈ)
- **0.99** (λ†’μ): High bias, low variance

**μμƒ ν¨κ³Ό:**
- λ‚®μ€ Ξ» β†’ λΉ λ¥Έ λ°μ‘, λ…Έμ΄μ¦μ— λ―Όκ°
- λ†’μ€ Ξ» β†’ μ•μ •μ , μ¥κΈ° λ³΄μƒ κ³ λ ¤

---

### 3. Training Epochs
κ° λ°°μΉμ— λ€ν• μ—…λ°μ΄νΈ λ°λ³µ νμ
- **10** (λ‚®μ): Under-fitting μ„ν—
- **15** (λ² μ΄μ¤λΌμΈ)
- **20** (λ†’μ): Over-fitting μ„ν—

**μμƒ ν¨κ³Ό:**
- μ μ€ epochs β†’ ν•™μµ λ¶€μ΅±
- λ§μ€ epochs β†’ κ³Όμ ν•©, λλ¦° μ²λ¦¬

---

### 4. Batch Size
ν• λ²μ— ν•™μµν•λ” μƒν” μ
- **8192** (μ‘μ): λ†’μ€ λ¶„μ‚°, λΉ λ¥Έ μ—…λ°μ΄νΈ
- **16384** (λ² μ΄μ¤λΌμΈ)
- **32768** (νΌ): λ‚®μ€ λ¶„μ‚°, μ•μ •μ  κ·Έλλ””μ–ΈνΈ

**μμƒ ν¨κ³Ό:**
- μ‘μ€ λ°°μΉ β†’ λ…Έμ΄μ§€ν• κ·Έλλ””μ–ΈνΈ, λΉ λ¥Έ λ°λ³µ
- ν° λ°°μΉ β†’ μ•μ •μ  ν•™μµ, λλ¦° λ°λ³µ

---

### 5. Network Architecture
μ‹ κ²½λ§μ ν¬κΈ°μ™€ μ©λ‰
- **[32, 32]** (small): λΉ λ¥΄μ§€λ§ ν‘ν„λ ¥ μ ν•
- **[64, 64]** (λ² μ΄μ¤λΌμΈ)
- **[128, 128]** (large): λ†’μ€ ν‘ν„λ ¥
- **[256, 256]** (xlarge): λ§¤μ° λ†’μ€ μ©λ‰

**μμƒ ν¨κ³Ό:**
- μ‘μ€ λ„¤νΈμ›ν¬ β†’ λΉ λ¥Έ ν•™μµ, μ ν•λ μ„±λ¥
- ν° λ„¤νΈμ›ν¬ β†’ λλ¦° ν•™μµ, λ†’μ€ μ„±λ¥ μ μ¬λ ¥

---

### 6. Value Function Coefficient
μ •μ±… μ†μ‹¤ λ€λΉ„ κ°€μΉ ν•¨μ μ†μ‹¤μ κ°€μ¤‘μΉ
- **0.005** (λ‚®μ): μ •μ±… μ„μ£Ό
- **0.01** (λ² μ΄μ¤λΌμΈ)
- **0.05** (λ†’μ)
- **0.1** (λ§¤μ° λ†’μ): κ°€μΉ ν•¨μ μ„μ£Ό

**μμƒ ν¨κ³Ό:**
- λ‚®μ€ κ³„μ β†’ μ •μ±… μµμ ν™” μ°μ„ 
- λ†’μ€ κ³„μ β†’ κ°€μΉ μ¶”μ • μ •ν™•λ„ ν–¥μƒ

---

### 7. Clip Parameter
PPOμ μ •μ±… μ—…λ°μ΄νΈ ν΄λ¦¬ν•‘ λ²”μ„
- **0.1** (λ‚®μ): λ³΄μμ  μ—…λ°μ΄νΈ
- **0.2** (λ² μ΄μ¤λΌμΈ)
- **0.3** (λ†’μ): κ³µκ²©μ  μ—…λ°μ΄νΈ

**μμƒ ν¨κ³Ό:**
- μ‘μ€ clip β†’ μ•μ •μ , λλ¦° μλ ΄
- ν° clip β†’ λΉ λ¥Έ μλ ΄, λ¶μ•μ • κ°€λ¥

---

### 8. Entropy Coefficient
νƒν—(exploration) μ¥λ ¤ μ •λ„
- **0.0** (λ² μ΄μ¤λΌμΈ): νƒν— μ—†μ
- **0.001** (λ‚®μ): μ•½κ°„μ νƒν—
- **0.01** (μ¤‘κ°„): μ λ‹Ήν• νƒν—
- **0.1** (λ†’μ): λ§μ€ νƒν—

**μμƒ ν¨κ³Ό:**
- 0.0 β†’ λΉ λ¥Έ μλ ΄, μ§€μ—­ μµμ ν™” μ„ν—
- λ†’μ€ κ°’ β†’ λ” λ§μ€ νƒν—, λλ¦° μλ ΄

---

### 9. μ΅°ν•© μ‹¤ν—

#### Fast Convergence
λΉ λ¥Έ μλ ΄μ„ μ„ν• κ³µκ²©μ  μ„¤μ •
```python
lr = 0.001
num_epochs = 20
clip_param = 0.3
```

#### Stable Learning
μ•μ •μ  ν•™μµμ„ μ„ν• λ³΄μμ  μ„¤μ •
```python
lr = 0.0001
lambda_ = 0.99
clip_param = 0.1
entropy_coeff = 0.001
```

#### Large Capacity
ν° μ©λ‰μΌλ΅ λ†’μ€ μ„±λ¥ μ¶”κµ¬
```python
train_batch_size = 32768
minibatch_size = 8192
fcnet_hiddens = [256, 256]
num_epochs = 10  # ν° λ°°μΉ λ³΄μƒ
```

---

## π“ μΈ΅μ • μ§€ν‘

### μ„±λ¥ (Performance)
- **episode_reward_mean**: μ—ν”Όμ†λ“λ³„ ν‰κ·  λ³΄μƒ
- **final_reward**: 10λ²μ§Έ iterationμ μµμΆ… λ³΄μƒ
- **mean_reward**: μ „μ²΄ iterationμ ν‰κ·  λ³΄μƒ

### μ•μ •μ„± (Stability)
- **reward_std**: 5ν μ‹¤ν–‰μ ν‘μ¤€νΈμ°¨
- **reward_cv**: λ³€λ™κ³„μ (CV = std/mean)
  - λ‚®μ„μλ΅ μ•μ •μ 
  - 0.1 λ―Έλ§: λ§¤μ° μ•μ •μ 
  - 0.1~0.2: λ³΄ν†µ
  - 0.2 μ΄μƒ: λ¶μ•μ •

### ν¨μ¨μ„± (Efficiency)
- **SPS**: Steps Per Second (μ²λ¦¬λ‰)
- **time_per_experiment**: μ‹¤ν—λ‹Ή μ†μ” μ‹κ°„

---

## π€ μ‹¤ν— μ‹¤ν–‰

### 1. μ‹¤ν— μν–‰
```bash
cd /home/com/reinforce-project4
python hyperparameter_stability_experiment.py
```

**μ‹¤ν–‰ μ •λ³΄:**
- μ΄ μ‹¤ν— μ: 23κ° (λ² μ΄μ¤λΌμΈ + 22κ° λ³€ν•)
- κ° μ‹¤ν—λ‹Ή trials: 5ν
- κ° trialλ‹Ή iterations: 10ν
- μ΄ ν•™μµ μ‹¤ν–‰: 115ν (23 Γ— 5)
- μμƒ μ†μ” μ‹κ°„: ~2-3μ‹κ°„ (ν™κ²½μ— λ”°λΌ λ‹¤λ¦„)

### 2. κ²°κ³Ό λ¶„μ„
```bash
python analyze_hyperparameter_experiments.py
```

**μ¶λ ¥:**
- μ„±λ¥ vs μ•μ •μ„± μ‚°μ λ„
- λ² μ΄μ¤λΌμΈ λ€λΉ„ κ°μ„ μ¨ λΉ„κµ
- νλΌλ―Έν„° κ·Έλ£Ήλ³„ ν¨κ³Ό λ¶„μ„
- ν•™μµ κ³΅μ„  λΉ„κµ
- μƒμ„Έ λ¶„μ„ λ³΄κ³ μ„ (Markdown)

---

## π“ μ¶λ ¥ νμΌ

```
hyperparameter_experiments/
β”β”€β”€ results/
β”‚   β”β”€β”€ hyperparameter_experiments_progress.json  # μ¤‘κ°„ μ €μ¥
β”‚   β”β”€β”€ hyperparameter_experiments_final.json     # μµμΆ… κ²°κ³Ό
β”‚   β”β”€β”€ HYPERPARAMETER_ANALYSIS_REPORT.md         # λ¶„μ„ λ³΄κ³ μ„
β”‚   β””β”€β”€ visualizations/
β”‚       β”β”€β”€ performance_vs_stability.png          # μ„±λ¥-μ•μ •μ„± μ‚°μ λ„
β”‚       β”β”€β”€ improvements_comparison.png           # κ°μ„ μ¨ λΉ„κµ
β”‚       β”β”€β”€ parameter_group_effects.png           # νλΌλ―Έν„° ν¨κ³Ό
β”‚       β””β”€β”€ learning_curves_comparison.png        # ν•™μµ κ³΅μ„ 
β””β”€β”€ README.md  # μ΄ νμΌ
```

---

## π“ λ¶„μ„ ν¬μΈνΈ

### 1. μ„±λ¥ λΉ„κµ
- μ–΄λ–¤ μ„¤μ •μ΄ κ°€μ¥ λ†’μ€ λ³΄μƒμ„ λ‹¬μ„±ν–λ”κ°€?
- λ² μ΄μ¤λΌμΈ λ€λΉ„ κ°μ„ μ¨μ€?

### 2. μ•μ •μ„± λΉ„κµ
- μ–΄λ–¤ μ„¤μ •μ΄ κ°€μ¥ μΌκ΄€λ κ²°κ³Όλ¥Ό λ³΄μ€λ”κ°€?
- λ³€λ™κ³„μ(CV)κ°€ κ°€μ¥ λ‚®μ€ μ„¤μ •μ€?

### 3. νλΌλ―Έν„° ν¨κ³Ό
- κ° νλΌλ―Έν„°κ°€ μ„±λ¥/μ•μ •μ„±μ— λ―ΈμΉλ” μν–¥
- νλΌλ―Έν„° κ°„ μƒνΈμ‘μ© ν¨κ³Ό

### 4. νΈλ μ΄λ“μ¤ν”„
- μ„±λ¥ vs μ•μ •μ„±
- μ„±λ¥ vs ν¨μ¨μ„± (SPS, μ‹κ°„)
- λΉ λ¥Έ μλ ΄ vs μµμΆ… μ„±λ¥

### 5. μ‹¤μ©μ  κ¶μ¥μ‚¬ν•­
- ν”„λ΅λ•μ… ν™κ²½μ— μ ν•©ν• μ„¤μ •
- μ—°κµ¬ λ©μ μ— μ ν•©ν• μ„¤μ •
- λ¦¬μ†μ¤ μ μ•½μ΄ μλ” ν™κ²½μ— μ ν•©ν• μ„¤μ •

---

## π” μμƒ κ²°κ³Ό

### Learning Rate
- **λ†’μ€ LR**: μ΄κΈ°μ— λΉ λ¥Έ ν–¥μƒ, λ¶μ•μ • κ°€λ¥
- **λ‚®μ€ LR**: μ•μ •μ μ΄μ§€λ§ μλ ΄ λλ¦Ό

### Batch Size
- **ν° λ°°μΉ**: μ•μ •μ  ν•™μµ, SPS μ €ν•
- **μ‘μ€ λ°°μΉ**: λ†’μ€ λ¶„μ‚°, λΉ λ¥Έ λ°λ³µ

### Network Size
- **ν° λ„¤νΈμ›ν¬**: λ†’μ€ μµμΆ… μ„±λ¥, λλ¦° ν•™μµ
- **μ‘μ€ λ„¤νΈμ›ν¬**: λΉ λ¥Έ ν•™μµ, μ ν•λ μ„±λ¥

### Combined Experiments
- **Fast Convergence**: λΉ λ¥Έ μ΄κΈ° ν–¥μƒ, λ¶μ•μ • κ°€λ¥
- **Stable Learning**: λλ¦¬μ§€λ§ μΌκ΄€λ ν–¥μƒ
- **Large Capacity**: λ†’μ€ μµμΆ… μ„±λ¥, λ¦¬μ†μ¤ μ§‘μ•½μ 

---

## π’΅ Tips

1. **μ‹¤ν— μ¤‘λ‹¨ μ‹**: Progress νμΌμ΄ μλ™ μ €μ¥λλ―€λ΅ λ³µκµ¬ κ°€λ¥
2. **λ©”λ¨λ¦¬ λ¶€μ΅± μ‹**: ν° λ°°μΉ/λ„¤νΈμ›ν¬ μ‹¤ν— μ μ™Έ κ³ λ ¤
3. **μ‹κ°„ λ‹¨μ¶•**: iterations μ μ¤„μ΄κΈ° (10 β†’ 5)
4. **GPU ν•„μ**: MuJoCo ν™κ²½ + ν° λ°°μΉ μ‹ GPU κ¶μ¥

---

## π“ μ°Έκ³  μλ£

- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [RLlib Documentation](https://docs.ray.io/en/latest/rllib/)
- [Hyperparameter Tuning Best Practices](https://spinningup.openai.com/en/latest/)

---

## β… μ²΄ν¬λ¦¬μ¤νΈ

μ‹¤ν— μ „:
- [ ] GPU μ‚¬μ© κ°€λ¥ ν™•μΈ
- [ ] μ¶©λ¶„ν• λ””μ¤ν¬ κ³µκ°„ (μµμ† 5GB)
- [ ] Ray λ° ν•„μ”ν• ν¨ν‚¤μ§€ μ„¤μΉ ν™•μΈ
- [ ] μμƒ μ†μ” μ‹κ°„ κ³ λ ¤

μ‹¤ν— μ¤‘:
- [ ] Progress νμΌ μ£ΌκΈ°μ  ν™•μΈ
- [ ] λ¦¬μ†μ¤ μ‚¬μ©λ‰ λ¨λ‹ν„°λ§
- [ ] μ΄μƒ μ‹¤ν— μ‹λ³„

μ‹¤ν— ν›„:
- [ ] κ²°κ³Ό νμΌ λ°±μ—…
- [ ] λ¶„μ„ μ¤ν¬λ¦½νΈ μ‹¤ν–‰
- [ ] μ‹κ°ν™” ν™•μΈ
- [ ] λ³΄κ³ μ„ κ²€ν† 

---

**Good Luck with Your Experiments! π€**
