"""
20227128 ê¹€ì§€í›ˆ

íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜ - ì‹ ë¢°ì„± ìˆëŠ” ì¡°í•© í…ŒìŠ¤íŠ¸

ë‹¨ì¼ íŒŒë¼ë¯¸í„° ì‹¤í—˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸í—Œ ê²€ì¦ëœ ì¡°í•© ì‹¤í—˜ ìˆ˜í–‰

ì°¸ê³  ë¬¸í—Œ:
    - PPO ì›ë…¼ë¬¸ (Schulman et al., 2017)
    - OpenAI Spinning Up
    - RLlib ê³µì‹ ë¬¸ì„œ
    - CleanRL MuJoCo ë²¤ì¹˜ë§ˆí¬

í™˜ê²½ ì„¤ì •:
    - ê¸°ë³¸ num_env_runners=10, ê· í˜• ì‹¤í—˜ì€ 16ê¹Œì§€ í™•ì¥
    - num_envs_per_env_runner=5 (RLlib ê¶Œì¥ 4-8 ë²”ìœ„)
    - ì´ 50~80ê°œ í™˜ê²½ ë™ì‹œ ì‹¤í–‰ (PPO ë…¼ë¬¸ 32-64 envs ê¶Œì¥ ìƒë‹¨ ëŒ€ë¹„ ì—¬ìœ ë¶„ í™•ë³´)
    - GPU ì‚¬ìš© (ê°€ëŠ¥ ì‹œ, ì—†ìœ¼ë©´ CPU ìë™ í´ë°±)
    - ê·¼ê±°: Schulman et al. 2017, CleanRL MuJoCo benchmark
    - ì˜ˆìƒ SPS: 25,000-35,000
    - ì˜ˆìƒ íš¨ìœ¨: 50-60%
"""

import argparse
import json
import shutil
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict

import ray
from ray.rllib.algorithms.ppo import PPOConfig
import numpy as np

PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from hyperparameter_experiments.resource_monitoring import (
    ResourceMonitor,
    aggregate_resource_stats,
)

try:
    import torch  # type: ignore
except ImportError:  # torch optional
    torch = None  # type: ignore

GPU_AVAILABLE = bool(torch) and torch.cuda.is_available()  # type: ignore
NUM_GPUS_PER_LEARNER = 1 if GPU_AVAILABLE else 0
RAY_RESULTS_DIR = PROJECT_ROOT / "ray_results"
RAY_RESULTS_DIR.mkdir(exist_ok=True)

# RLlib PPO ê¸°ë³¸ê°’ì„ í•œ ë²ˆë§Œ ì¶”ì¶œí•´ë‘”ë‹¤.
_PPO_DEFAULT = PPOConfig()
PPO_OPTIONAL_DEFAULTS: Dict[str, Any] = {
    'clip_param': _PPO_DEFAULT.clip_param,
    'vf_clip_param': _PPO_DEFAULT.vf_clip_param,
    'entropy_coeff': _PPO_DEFAULT.entropy_coeff,
    'use_kl_loss': getattr(_PPO_DEFAULT, 'use_kl_loss', False),
    'kl_coeff': _PPO_DEFAULT.kl_coeff,
    'kl_target': _PPO_DEFAULT.kl_target,
    'grad_clip': _PPO_DEFAULT.grad_clip,
    'gamma': _PPO_DEFAULT.gamma,
    'use_gae': _PPO_DEFAULT.use_gae,
    'use_critic': _PPO_DEFAULT.use_critic,
}


def _optional_value(config: Dict[str, Any], key: str):
    if key in config:
        return config[key]
    return PPO_OPTIONAL_DEFAULTS[key]


def _load_overrides(overrides_arg: str | None) -> Dict[str, Any]:
    """JSON ë¬¸ìì—´ ë˜ëŠ” íŒŒì¼ ê²½ë¡œë¡œë¶€í„° override dictë¥¼ ë¡œë“œí•œë‹¤."""
    if not overrides_arg:
        return {}

    candidate = Path(overrides_arg)
    if candidate.exists():
        with open(candidate, 'r', encoding='utf-8') as f:
            data = json.load(f)
    else:
        data = json.loads(overrides_arg)

    if not isinstance(data, dict):
        raise ValueError("Baseline overridesëŠ” dict í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.")
    return data


def _parse_args():
    parser = argparse.ArgumentParser(description="Run PPO hyperparameter combination experiments")
    parser.add_argument(
        "--baseline-overrides",
        type=str,
        default=None,
        help="JSON string or path to JSON file with additional baseline parameters",
    )
    parser.add_argument(
        "--auto-yes",
        action="store_true",
        help="Skip interactive confirmation",
    )
    return parser.parse_args()


def get_baseline_config(extra_overrides: Dict[str, Any] | None = None):
    """ë² ì´ìŠ¤ë¼ì¸ ì„¤ì • (ê³ ì • íŒŒë¼ë¯¸í„°)

    extra_overridesë¥¼ í†µí•´ ì›í•˜ëŠ” íŒŒë¼ë¯¸í„°ë§Œ ê°„ë‹¨íˆ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤.
    """
    config = {
        'lambda_': 0.95,
        'lr': 0.0003,
        'num_epochs': 15,
        'train_batch_size': 32 * 512,  # 16384
        'minibatch_size': 4096,
        'vf_loss_coeff': 0.01,
        'fcnet_hiddens': [64, 64],
        'fcnet_activation': 'tanh',
        'vf_share_layers': False,
    }

    if extra_overrides:
        config.update(extra_overrides)

    return config


def get_combination_experiments(baseline_overrides: Dict[str, Any] | None = None):
    """ìš”ì²­ëœ 5ê°œ ì‹¤í—˜ í™˜ê²½ ì„¸íŠ¸"""
    baseline = get_baseline_config(baseline_overrides)

    def with_common_overrides(**overrides):
        cfg = baseline.copy()
        cfg.update(overrides)
        return cfg

    return [
        {
            'name': 'baseline_default',
            'description': 'Exact baseline hyperparameters (no overrides)',
            'category': 'baseline',
            'rationale': 'Pure reference run using untouched defaults',
            'expected': 'Matches single-parameter baseline performance',
            'params': baseline.copy(),
        },
        {
            'name': 'baseline_conservative',
            'description': 'Baseline (conservative defaults)',
            'category': 'baseline',
            'rationale': 'Reference configuration for comparisons',
            'expected': 'Stable but slower convergence',
            'params': with_common_overrides(
                clip_param=0.2,
                gamma=0.99,
                kl_target=0.01,
                optimizer_config={'sgd_momentum': 0.0},
            )
        },
        {
            'name': 'aggressive_exploration',
            'description': 'Aggressive exploration with scheduled entropy/clip',
            'category': 'speed',
            'rationale': 'Fast initial learning via high clip & entropy',
            'expected': 'Fast early gains, late instability risk',
            'params': with_common_overrides(
                clip_param=0.3,
                gamma=0.90,
                kl_target=0.2,
                entropy_coeff=0.01,
                optimizer_config={'sgd_momentum': 0.9},
            )
        },
        {
            'name': 'stable_conservative',
            'description': 'Stable conservative setting with weight decay',
            'category': 'stability',
            'rationale': 'Maximum stability focus',
            'expected': 'Slowest but most stable convergence',
            'params': with_common_overrides(
                clip_param=0.1,
                gamma=0.99,
                kl_target=0.01,
                entropy_coeff=0.0,
                optimizer_config={'sgd_momentum': 0.99, 'weight_decay': 1e-4},
            )
        },
        {
            'name': 'balanced_high_momentum',
            'description': 'Balanced config with high momentum and more workers',
            'category': 'balanced',
            'rationale': 'Practical balance of speed and stability',
            'expected': 'Fast convergence while remaining stable',
            'params': with_common_overrides(
                clip_param=0.25,
                gamma=0.95,
                kl_target=0.02,
                entropy_coeff=0.001,
                optimizer_config={'sgd_momentum': 0.95},
            )
        },
        {
            'name': 'kl_focused',
            'description': 'KL-focused training with stronger penalties',
            'category': 'stability',
            'rationale': 'Tight KL and VF regularisation',
            'expected': 'Most accurate value estimates',
            'params': with_common_overrides(
                clip_param=0.25,
                gamma=0.95,
                kl_target=0.1,
                kl_coeff=0.1,
                entropy_coeff=0.001,
                optimizer_config={'sgd_momentum': 0.9},
                vf_loss_coeff=0.5,
            )
        },
    ]


DEFAULT_RUNNER_CONFIG = {
    'num_env_runners': 10,
    'num_envs_per_env_runner': 5,
    'num_cpus_per_env_runner': 1,
}


def _scheduled_value(schedule, current_step):
    if not schedule:
        return None
    value = schedule[0][1]
    for step, val in schedule:
        if current_step >= step:
            value = val
        else:
            break
    return value


def _foreach_policy(algo, fn: Callable):
    if hasattr(algo, 'env_runner_group') and algo.env_runner_group is not None:
        def _wrapper(policy, policy_id):
            fn(policy)
        algo.env_runner_group.foreach_policy(_wrapper)
        return

    workers = getattr(algo, 'workers', None)
    if workers is not None and hasattr(workers, 'foreach_policy'):
        workers.foreach_policy(lambda policy, *_: fn(policy))
        return

    default_policy = algo.get_policy() if hasattr(algo, 'get_policy') else None
    if default_policy is not None:
        fn(default_policy)


def _apply_clip_param(algo, new_value):
    if new_value is None:
        return
    algo.config['clip_param'] = new_value

    def _set(policy):
        policy.config['clip_param'] = new_value

    _foreach_policy(algo, _set)


def _build_model_config(config_dict):
    base_model = {
        "fcnet_hiddens": config_dict['fcnet_hiddens'],
        "fcnet_activation": config_dict['fcnet_activation'],
        "vf_share_layers": config_dict['vf_share_layers'],
    }

    initial_log_std = config_dict.get('initial_log_std')
    model_overrides = config_dict.get('model_overrides') or {}
    overrides_copy = dict(model_overrides)

    # Legacy format: {'action_dist_config': {'initial_log_std': x}}
    action_dist_cfg = overrides_copy.pop('action_dist_config', None)
    if isinstance(action_dist_cfg, dict) and 'initial_log_std' in action_dist_cfg:
        initial_log_std = action_dist_cfg['initial_log_std']

    # Allow direct usage via overrides as well.
    if 'initial_log_std' in overrides_copy:
        initial_log_std = overrides_copy.pop('initial_log_std')

    base_model.update(overrides_copy)
    return base_model, initial_log_std


def _apply_initial_log_std(algo, log_std_value):
    if log_std_value is None or torch is None:  # type: ignore
        return

    log_std_value = float(log_std_value)

    def _setter(policy):
        action_space = getattr(policy, 'action_space', None)
        if not action_space or not getattr(action_space, 'shape', None):
            return
        action_dim = int(np.prod(action_space.shape))
        model = policy.model

        # free_log_std uses dedicated parameter tensor.
        if getattr(model, 'free_log_std', False) and hasattr(model, '_append_free_log_std'):
            append_layer = getattr(model, '_append_free_log_std')
            log_std_tensor = getattr(append_layer, 'log_std', None)
            if log_std_tensor is None:
                return
            with torch.no_grad():  # type: ignore
                log_std_tensor.fill_(log_std_value)
            return

        logits = getattr(model, '_logits', None)
        bias = getattr(logits, 'bias', None) if logits else None
        if bias is None or bias.shape[0] < action_dim * 2:
            return

        with torch.no_grad():  # type: ignore
            bias[action_dim:action_dim * 2] = log_std_value

    _foreach_policy(algo, _setter)
    print(f"    â†º initial log_std set to {log_std_value:.3f}")


def _resolve_checkpoint_path(raw_checkpoint):
    checkpoint_obj = getattr(raw_checkpoint, "checkpoint", None)
    if checkpoint_obj:
        path_attr = getattr(checkpoint_obj, "path", None)
        if path_attr:
            return Path(path_attr)

    if isinstance(raw_checkpoint, Path):
        return raw_checkpoint
    if isinstance(raw_checkpoint, str):
        return Path(raw_checkpoint)

    path_attr = getattr(raw_checkpoint, "path", None)
    if path_attr:
        return Path(path_attr)

    to_uri = getattr(raw_checkpoint, "to_uri", None)
    if callable(to_uri):
        uri = to_uri()
        if isinstance(uri, str) and uri.startswith("file://"):
            return Path(uri[7:])
    return None


def print_system_info():
    """ì‹œìŠ¤í…œ í™˜ê²½ ì •ë³´ ì¶œë ¥"""
    import platform
    import psutil
    
    print("="*80)
    print("íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜ - ì‹œìŠ¤í…œ í™˜ê²½ ì •ë³´")
    print("="*80)
    
    # ê¸°ë³¸ ì •ë³´
    print(f"\n[ì‹œìŠ¤í…œ ì •ë³´]")
    print(f"  ìš´ì˜ì²´ì œ: {platform.system()} {platform.release()}")
    print(f"  Python: {platform.python_version()}")
    print(f"  Ray: {ray.__version__}")
    
    # CPU ì •ë³´
    print(f"\n[CPU ì •ë³´]")
    print(f"  í”„ë¡œì„¸ì„œ: {platform.processor()}")
    print(f"  ë¬¼ë¦¬ ì½”ì–´: {psutil.cpu_count(logical=False)}ê°œ")
    print(f"  ë…¼ë¦¬ ì½”ì–´: {psutil.cpu_count(logical=True)}ê°œ")
    
    # ë©”ëª¨ë¦¬ ì •ë³´
    memory = psutil.virtual_memory()
    print(f"\n[ë©”ëª¨ë¦¬ ì •ë³´]")
    print(f"  ì´ ë©”ëª¨ë¦¬: {memory.total / (1024**3):.1f}GB")
    print(f"  ì‚¬ìš© ê°€ëŠ¥: {memory.available / (1024**3):.1f}GB")
    
    # GPU ì •ë³´
    print(f"\n[GPU ì •ë³´]")
    if GPU_AVAILABLE:
        print(f"  CUDA ì‚¬ìš© ê°€ëŠ¥: Yes")
        print(f"  CUDA ë²„ì „: {torch.version.cuda}")  # type: ignore[attr-defined]
        print(f"  GPU ê°œìˆ˜: {torch.cuda.device_count()}ê°œ")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            props = torch.cuda.get_device_properties(i)
            print(f"    ë©”ëª¨ë¦¬: {props.total_memory / (1024**3):.1f}GB")
    elif torch is not None:
        print(f"  CUDA ì‚¬ìš© ê°€ëŠ¥: No (PyTorch ì„¤ì¹˜ë¨)")
    else:
        print(f"  CUDA ì‚¬ìš© ê°€ëŠ¥: PyTorch ë¯¸ì„¤ì¹˜")
    
    # ë³‘ë ¬í™” ì„¤ì •
    print(f"\n[ë³‘ë ¬í™” ê¸°ë³¸ ì„¤ì •]")
    print(f"  num_env_runners: {DEFAULT_RUNNER_CONFIG['num_env_runners']}")
    print(f"  num_envs_per_env_runner: {DEFAULT_RUNNER_CONFIG['num_envs_per_env_runner']}")
    total_envs = DEFAULT_RUNNER_CONFIG['num_env_runners'] * DEFAULT_RUNNER_CONFIG['num_envs_per_env_runner']
    print(f"  ì´ í™˜ê²½ ìˆ˜: {total_envs}")
    learner_desc = "1 (GPU ì‚¬ìš©)" if GPU_AVAILABLE else "1 (CPU ëª¨ë“œ)"
    print(f"  num_learners: {learner_desc}")
    if not GPU_AVAILABLE:
        print("  âš ï¸ GPU ë¯¸íƒì§€ â†’ CPU í•™ìŠµ (ëŠë¦´ ìˆ˜ ìˆìŒ)")
    print(f"  ê·¼ê±°: CleanRL ë²¤ì¹˜ë§ˆí¬ + PPO ë…¼ë¬¸ (32-64 envs ê¶Œì¥, ê¸°ë³¸ 10 runners)")
    print(f"  ì˜ˆìƒ SPS: 25,000-35,000")
    print(f"  ì˜ˆìƒ íš¨ìœ¨: 50-60%")
    
    print("="*80)


def run_single_trial(config_dict, exp_name, trial_num, num_iterations=10, save_checkpoint=False):
    """ë‹¨ì¼ ì‹œí–‰ ì‹¤í–‰
    
    Args:
        config_dict: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        exp_name: ì‹¤í—˜ ì´ë¦„
        trial_num: ì‹œí–‰ ë²ˆí˜¸
        num_iterations: ë°˜ë³µ íšŸìˆ˜
        save_checkpoint: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì—¬ë¶€
    """
    runner_cfg = DEFAULT_RUNNER_CONFIG.copy()

    clip_schedule = config_dict.get('clip_param_schedule')
    initial_clip = _scheduled_value(clip_schedule, 0) or _optional_value(config_dict, 'clip_param')

    model_config, initial_log_std = _build_model_config(config_dict)

    optimizer_config = config_dict.get('optimizer_config')

    training_kwargs = dict(
        lambda_=config_dict['lambda_'],
        lr=config_dict['lr'],
        num_epochs=config_dict['num_epochs'],
        train_batch_size=config_dict['train_batch_size'],
        minibatch_size=config_dict['minibatch_size'],
        vf_loss_coeff=config_dict['vf_loss_coeff'],
        clip_param=initial_clip,
        vf_clip_param=_optional_value(config_dict, 'vf_clip_param'),
        entropy_coeff=_optional_value(config_dict, 'entropy_coeff'),
        use_kl_loss=_optional_value(config_dict, 'use_kl_loss'),
        kl_coeff=_optional_value(config_dict, 'kl_coeff'),
        kl_target=_optional_value(config_dict, 'kl_target'),
        grad_clip=_optional_value(config_dict, 'grad_clip'),
        gamma=_optional_value(config_dict, 'gamma'),
        use_gae=_optional_value(config_dict, 'use_gae'),
        use_critic=_optional_value(config_dict, 'use_critic'),
        model=model_config,
    )

    if config_dict.get('entropy_coeff_schedule'):
        training_kwargs['entropy_coeff_schedule'] = config_dict['entropy_coeff_schedule']

    config = (
        PPOConfig()
        .environment("HalfCheetah-v5")
        .api_stack(
            enable_rl_module_and_learner=False,
            enable_env_runner_and_connector_v2=False,
        )
    )
    if optimizer_config:
        config = config.training(**training_kwargs, optimizer=optimizer_config)
    else:
        config = config.training(**training_kwargs)

    config = (
        config
        .learners(num_learners=1, num_gpus_per_learner=NUM_GPUS_PER_LEARNER)
        .debugging(
            seed=20227128 + trial_num,
            log_level="WARN",
            log_sys_usage=True,
        )
        .env_runners(
            num_env_runners=runner_cfg['num_env_runners'],
            num_envs_per_env_runner=runner_cfg['num_envs_per_env_runner'],
            num_cpus_per_env_runner=runner_cfg['num_cpus_per_env_runner'],
        )
        .evaluation(
            evaluation_num_env_runners=1,
            evaluation_interval=0,
            evaluation_duration=5
        )
    )

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    trial_logdir = RAY_RESULTS_DIR / f"{exp_name}_trial{trial_num + 1}_{timestamp}"
    trial_logdir.mkdir(parents=True, exist_ok=True)

    def _logger_creator(cfg):
        from ray.tune.logger import UnifiedLogger
        return UnifiedLogger(cfg, str(trial_logdir))

    # ì•Œê³ ë¦¬ì¦˜ ë¹Œë“œ
    algo = config.build(logger_creator=_logger_creator)
    
    results = []
    start_time = time.time()
    checkpoint_path = None

    _apply_clip_param(algo, initial_clip)
    _apply_initial_log_std(algo, initial_log_std)
    total_env_steps = 0
    monitor_samples = []
    
    try:
        for iteration in range(num_iterations):
            scheduled_clip = _scheduled_value(clip_schedule, total_env_steps)
            current_clip = algo.config.get('clip_param', initial_clip)
            if scheduled_clip is not None and abs(scheduled_clip - current_clip) > 1e-6:
                _apply_clip_param(algo, scheduled_clip)
                print(f"    â†º clip_param schedule applied: {scheduled_clip:.3f} (steps={total_env_steps})")
            
            monitor = ResourceMonitor()
            monitor.start()
            iter_start = time.time()
            result = algo.train()
            iter_time = time.time() - iter_start
            monitor.stop()
            monitor_stats = monitor.get_stats()
            if monitor_stats:
                monitor_samples.append(monitor_stats)
            
            # ë©”íŠ¸ë¦­ ì¶”ì¶œ
            env_runners = result.get('env_runners', {})
            episode_reward_mean = env_runners.get('episode_reward_mean', 
                                                  env_runners.get('episode_return_mean', 0))
            
            # í•™ìŠµ ë‹¨ê³„ ì¶”ì¶œ (ì¤‘ìš”!)
            def _first_positive(*values):
                for val in values:
                    if isinstance(val, (int, float)) and val > 0:
                        return float(val)
                return 0.0

            num_env_steps_trained = _first_positive(
                result.get('num_env_steps_trained'),
                result.get('num_env_steps_trained_this_iter'),
                result.get('num_env_steps_trained_total'),
                result.get('counters', {}).get('num_env_steps_trained'),
                env_runners.get('num_env_steps_trained'),
                env_runners.get('num_env_steps_sampled'),
                result.get('num_env_steps_sampled')
            )
            
            # ì²« iterationì—ì„œ í•™ìŠµ ê²€ì¦
            if iteration == 0:
                if num_env_steps_trained == 0:
                    raise RuntimeError(
                        f"âŒ í•™ìŠµì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! "
                        f"num_env_steps_trained=0\n"
                        f"learner ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš” (num_learners >= 1 í•„ìš”)"
                    )
                else:
                    print(f"    âœ“ í•™ìŠµ ì‹œì‘ í™•ì¸: {num_env_steps_trained} steps trained")
            
            # SPS ê³„ì‚°
            num_env_steps = 0
            for key in ['num_env_steps_sampled', 'num_env_steps_sampled_this_iter']:
                if key in env_runners:
                    num_env_steps = env_runners[key]
                    break
            if num_env_steps == 0:
                num_env_steps = config_dict['train_batch_size']
            
            sps = num_env_steps / iter_time if iter_time > 0 else 0
            
            metrics = {
                'iteration': iteration + 1,
                'episode_reward_mean': float(episode_reward_mean),
                'episode_reward_min': float(env_runners.get('episode_reward_min', 0)),
                'episode_reward_max': float(env_runners.get('episode_reward_max', 0)),
                'episode_len_mean': float(env_runners.get('episode_len_mean', 0)),
                'num_env_steps_sampled': int(num_env_steps),
                'num_env_steps_trained': int(num_env_steps_trained),
                'time_this_iter_s': float(iter_time),
                'sps': float(sps),
                'resource_monitor': monitor_stats,
            }
            
            results.append(metrics)
            total_env_steps = max(total_env_steps, int(result.get('num_env_steps_sampled', total_env_steps)))
            
            print(f"    Iter {iteration + 1}/{num_iterations}: "
                  f"Reward={metrics['episode_reward_mean']:.2f}, "
                  f"Trained={num_env_steps_trained}, "
                  f"Time={iter_time:.2f}s, "
                  f"SPS={sps:.0f}")
            if monitor_stats:
                cpu_avg = monitor_stats.get('cpu_avg')
                cpu_max = monitor_stats.get('cpu_max')
                ram_avg = monitor_stats.get('ram_avg')
                gpu_avg = monitor_stats.get('gpu_avg')
                vram_avg = monitor_stats.get('vram_avg_mb')
                monitor_msg = "      Resource â‡¨ "
                if cpu_avg is not None:
                    monitor_msg += f"CPU {cpu_avg:.1f}% avg / {cpu_max:.1f}% max"
                if ram_avg is not None:
                    monitor_msg += f", RAM {ram_avg:.1f}%"
                if gpu_avg is not None:
                    monitor_msg += f", GPU {gpu_avg:.1f}%"
                if vram_avg is not None:
                    monitor_msg += f", VRAM {vram_avg:.0f}MB"
                print(monitor_msg)
    
        
        # ë§ˆì§€ë§‰ iteration ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì˜µì…˜)
        if save_checkpoint and iteration == num_iterations - 1:
            raw_checkpoint = algo.save()
            checkpoint_src = _resolve_checkpoint_path(raw_checkpoint)
            repo_checkpoint_dir = trial_logdir / f"checkpoint_iter{iteration + 1}"
            if checkpoint_src and checkpoint_src.exists():
                if repo_checkpoint_dir.exists():
                    shutil.rmtree(repo_checkpoint_dir)
                shutil.copytree(checkpoint_src, repo_checkpoint_dir)
                checkpoint_path = str(repo_checkpoint_dir)
            else:
                checkpoint_path = str(checkpoint_src or raw_checkpoint)
            print(f"    ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path}")
    
    finally:
        algo.stop()
    
    total_time = time.time() - start_time
    resource_summary = aggregate_resource_stats(monitor_samples)
    
    return {
        'trial_num': trial_num,
        'iterations': results,
        'total_time': float(total_time),
        'final_reward': float(results[-1]['episode_reward_mean']) if results else 0.0,
        'checkpoint_path': checkpoint_path,
        'resource_summary': resource_summary,
    }


def run_experiment(exp_config, num_trials=5, num_iterations=10, save_checkpoints=False):
    """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ (ì—¬ëŸ¬ ì‹œí–‰)
    
    Args:
        exp_config: ì‹¤í—˜ ì„¤ì •
        num_trials: ì‹œí–‰ íšŸìˆ˜
        num_iterations: ë°˜ë³µ íšŸìˆ˜
        save_checkpoints: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì—¬ë¶€ (ë§ˆì§€ë§‰ trialë§Œ)
    """
    exp_name = exp_config['name']
    print(f"\n{'='*80}")
    print(f"ì‹¤í—˜: {exp_name}")
    print(f"ì„¤ëª…: {exp_config['description']}")
    print(f"ì¹´í…Œê³ ë¦¬: {exp_config['category']}")
    print(f"ê·¼ê±°: {exp_config['rationale']}")
    if 'expected' in exp_config:
        print(f"ì˜ˆìƒ ê²°ê³¼: {exp_config['expected']}")
    print(f"{'='*80}")
    
    trials_results = []
    for trial in range(num_trials):
        print(f"\n  Trial {trial + 1}/{num_trials}")
        print(f"  {'='*50}")
        
        try:
            # ë§ˆì§€ë§‰ trialë§Œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_this_trial = save_checkpoints and (trial == num_trials - 1)
            trial_result = run_single_trial(
                exp_config['params'],
                exp_name,
                trial,
                num_iterations,
                save_checkpoint=save_this_trial
            )
            trials_results.append(trial_result)
            
        except Exception as e:
            print(f"  [ERROR] Trial {trial + 1} failed: {str(e)}")
            continue
    
    if not trials_results:
        print(f"  [WARNING] No successful trials for {exp_name}")
        return None
    
    # í†µê³„ ê³„ì‚°
    final_rewards = [t['final_reward'] for t in trials_results]
    all_iterations = [t['iterations'] for t in trials_results]
    
    # ê° iterationë³„ í‰ê·  ê³„ì‚°
    mean_rewards_per_iter = []
    if all_iterations:
        num_iters = len(all_iterations[0])
        for i in range(num_iters):
            iter_rewards = [trial[i]['episode_reward_mean'] for trial in all_iterations if len(trial) > i]
            if iter_rewards:
                mean_rewards_per_iter.append(np.mean(iter_rewards))
    
    # SPS í†µê³„
    all_sps = []
    for trial in trials_results:
        for iteration in trial['iterations']:
            if 'sps' in iteration and iteration['sps'] > 0:
                all_sps.append(iteration['sps'])
    
    statistics = {
        'final_reward_mean': float(np.mean(final_rewards)),
        'final_reward_std': float(np.std(final_rewards)),
        'final_reward_min': float(np.min(final_rewards)),
        'final_reward_max': float(np.max(final_rewards)),
        'final_reward_cv': float(np.std(final_rewards) / abs(np.mean(final_rewards))) if np.mean(final_rewards) != 0 else 0.0,
        'mean_rewards_per_iter': [float(x) for x in mean_rewards_per_iter],
        'sps_mean': float(np.mean(all_sps)) if all_sps else 0.0,
        'sps_std': float(np.std(all_sps)) if all_sps else 0.0,
    }

    resource_samples = [
        iteration.get('resource_monitor')
        for trial in trials_results
        for iteration in trial['iterations']
        if iteration.get('resource_monitor')
    ]
    trial_resource_summaries = [trial.get('resource_summary') for trial in trials_results if trial.get('resource_summary')]
    resource_usage = aggregate_resource_stats(resource_samples or trial_resource_summaries)
    if resource_usage:
        statistics['resource_usage'] = resource_usage
    
    print(f"\n  {'='*50}")
    print(f"  ìµœì¢… í†µê³„:")
    print(f"    í‰ê·  ë³´ìƒ: {statistics['final_reward_mean']:.2f} Â± {statistics['final_reward_std']:.2f}")
    print(f"    ë³€ë™ê³„ìˆ˜(CV): {statistics['final_reward_cv']:.4f}")
    print(f"    ë²”ìœ„: [{statistics['final_reward_min']:.2f}, {statistics['final_reward_max']:.2f}]")
    print(f"    í‰ê·  SPS: {statistics['sps_mean']:.0f} Â± {statistics['sps_std']:.0f}")
    if 'resource_usage' in statistics:
        ru = statistics['resource_usage']
        cpu_msg = f"CPU {ru.get('cpu_avg', 0):.1f}% avg"
        if 'cpu_max' in ru:
            cpu_msg += f" / {ru['cpu_max']:.1f}% max"
        print(f"    ë¦¬ì†ŒìŠ¤: {cpu_msg}")
        if 'gpu_avg' in ru:
            gpu_msg = f"GPU {ru['gpu_avg']:.1f}% avg"
            if 'gpu_max' in ru:
                gpu_msg += f" / {ru['gpu_max']:.1f}% max"
            print(f"            {gpu_msg}, VRAM {ru.get('vram_avg_mb', 0):.0f}MB avg")
    
    return {
        'name': exp_name,
        'description': exp_config['description'],
        'category': exp_config['category'],
        'rationale': exp_config['rationale'],
        'expected': exp_config.get('expected', ''),
        'params': exp_config['params'],
        'trials': trials_results,
        'statistics': statistics,
        'resource_summary': resource_usage,
    }


def main(cli_args=None):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    if cli_args is None:
        cli_args = _parse_args()

    try:
        baseline_overrides = _load_overrides(getattr(cli_args, 'baseline_overrides', None))
    except Exception as exc:  # pragma: no cover - CLI ìœ íš¨ì„± ê²€ì‚¬ìš©
        print(f"[ERROR] baseline overrides íŒŒì‹± ì‹¤íŒ¨: {exc}")
        return

    print("\n" + "="*80)
    print("íŒŒë¼ë¯¸í„° ì¡°í•© ì‹¤í—˜ ì‹œì‘")
    print("20227128 ê¹€ì§€í›ˆ")
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()
    
    # ì‹¤í—˜ ì„¤ì • ë¡œë“œ
    experiments = get_combination_experiments(baseline_overrides)
    
    print(f"\nì´ {len(experiments)}ê°œì˜ ì¡°í•© ì‹¤í—˜ ì˜ˆì •")
    print(f"ê° ì‹¤í—˜ë‹¹ 5íšŒ ì‹œí–‰, ì‹œí–‰ë‹¹ 10íšŒ ë°˜ë³µ")
    total_envs = DEFAULT_RUNNER_CONFIG['num_env_runners'] * DEFAULT_RUNNER_CONFIG['num_envs_per_env_runner']
    print(f"ë³‘ë ¬í™”: ê¸°ë³¸ {DEFAULT_RUNNER_CONFIG['num_env_runners']} runners Ã— {DEFAULT_RUNNER_CONFIG['num_envs_per_env_runner']} envs = {total_envs}ê°œ í™˜ê²½ (ê· í˜• ì‹¤í—˜ì€ 16 runners)")
    learner_desc = "num_learners=1 (GPU ì‚¬ìš©)" if GPU_AVAILABLE else "num_learners=1 (CPU ëª¨ë“œ)"
    print(f"í•™ìŠµ: {learner_desc}")
    print(f"ê·¼ê±°: PPO ë…¼ë¬¸ 32-64 envs, CleanRL 8 workers â†’ ê¸°ë³¸ 10 runnersë¡œ í™•ì¥")
    print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 20-25ë¶„\n")
    
    # ì‚¬ìš©ì í™•ì¸
    if getattr(cli_args, 'auto_yes', False):
        response = 'yes'
        print("ì‹¤í—˜ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): yes (auto)")
    else:
        response = input("ì‹¤í—˜ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (yes/no): ")
    if response.lower() != 'yes':
        print("ì‹¤í—˜ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
        return
    
    # Ray ì´ˆê¸°í™”
    if not ray.is_initialized():
        ray.init(ignore_reinit_error=True)
    
    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    results_dir = Path(__file__).parent / "results"
    results_dir.mkdir(exist_ok=True)
    
    # ì‹¤í—˜ ì‹¤í–‰
    all_results = {
        'metadata': {
            'start_time': datetime.now().isoformat(),
            'num_experiments': len(experiments),
            'num_trials_per_experiment': 5,
            'num_iterations_per_trial': 10,
            'parallelization': {
                'default_num_env_runners': DEFAULT_RUNNER_CONFIG['num_env_runners'],
                'num_envs_per_env_runner': DEFAULT_RUNNER_CONFIG['num_envs_per_env_runner'],
                'total_envs_default': total_envs,
                'special_cases': {},
                'num_learners': 1,
                'num_gpus_per_learner': NUM_GPUS_PER_LEARNER,
                'rationale': 'PPO paper: 32-64 parallel envs, CleanRL: 8 workers (default expanded to 10 runners)'
            }
        },
        'experiments': []
    }
    
    start_time = time.time()
    
    for i, exp_config in enumerate(experiments):
        print(f"\n{'#'*80}")
        print(f"ì§„í–‰ ìƒí™©: ì‹¤í—˜ {i + 1}/{len(experiments)}")
        print(f"{'#'*80}")
        
        # baseline ì‹¤í—˜ë§Œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        save_ckpt = (exp_config['name'] == 'baseline_conservative')
        exp_result = run_experiment(exp_config, num_trials=5, num_iterations=10, 
                                   save_checkpoints=save_ckpt)
        
        if exp_result:
            all_results['experiments'].append(exp_result)
        
        # ì¤‘ê°„ ì €ì¥
        with open(results_dir / "combination_experiments_progress.json", 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        print(f"\n[ì§„í–‰ ì €ì¥: {results_dir}/combination_experiments_progress.json]")
    
    total_time = time.time() - start_time
    all_results['metadata']['end_time'] = datetime.now().isoformat()
    all_results['metadata']['total_time_seconds'] = float(total_time)
    all_results['metadata']['total_time_minutes'] = float(total_time / 60)
    
    # ìµœì¢… ì €ì¥
    with open(results_dir / "combination_experiments_final.json", 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print("\n" + "="*80)
    print("ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")
    print(f"ì´ ì†Œìš” ì‹œê°„: {total_time / 60:.1f}ë¶„ ({total_time / 3600:.2f}ì‹œê°„)")
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {results_dir}")
    print(f"  - ìµœì¢… ê²°ê³¼: combination_experiments_final.json")
    print(f"  - ì§„í–‰ ê¸°ë¡: combination_experiments_progress.json")
    print("="*80)

    if not all_results['experiments']:
        print("\n[ERROR] ë‹¨ í•œ ê°œì˜ trialë„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("       ìœ„ì— í‘œì‹œëœ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë¨¼ì € í•´ê²°í•˜ì„¸ìš” (ì˜ˆ: MuJoCo ë¯¸ì„¤ì¹˜, GPU ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ë“±).")
        print("       ë¬¸ì œê°€ ì§€ì†ë˜ë©´ --cpu-only í™˜ê²½ì„ í™•ì¸í•˜ê±°ë‚˜ requirementsë¥¼ ì¬ì„¤ì¹˜í•˜ì‹­ì‹œì˜¤.")
        ray.shutdown()
        return
    
    # ê°„ë‹¨í•œ ìš”ì•½
    print("\nì‹¤í—˜ ìš”ì•½:")
    print(f"{'='*80}")
    print(f"{'ì‹¤í—˜ëª…':<25} {'ìµœì¢… ë³´ìƒ':<20} {'CV':<10} {'SPS':<10}")
    print(f"{'-'*80}")
    
    for exp in all_results['experiments']:
        stats = exp['statistics']
        print(f"{exp['name']:<25} "
            f"{stats['final_reward_mean']:>7.2f} Â± {stats['final_reward_std']:<7.2f} "
            f"{stats['final_reward_cv']:>8.4f} "
            f"{stats['sps_mean']:>9.0f}")
    
    print(f"{'='*80}")
    
    # Ray ì¢…ë£Œ
    ray.shutdown()


if __name__ == "__main__":
    main(_parse_args())
