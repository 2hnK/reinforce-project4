# 하이퍼파라미터 안정성 및 성능 분석 보고서

**학번:** 20227128 김지훈

**날짜:** 2025-11-16

---

## 1. 실험 개요

- **총 실험 수:** 15개 (베이스라인 제외)
- **실험당 시행 횟수:** 5회
- **시행당 반복 횟수:** 10회
- **환경:** HalfCheetah-v5 (MuJoCo)
- **알고리즘:** PPO (Proximal Policy Optimization)
- **총 실험 시간:** 약 217분 (3.6시간)

## 2. 베이스라인 설정

### 고정 학습 파라미터 (실험에서 변경하지 않음)
```python
lambda_ = 0.95              # GAE lambda
lr = 0.0003                 # 학습률
num_epochs = 15             # 에폭 수
train_batch_size = 16384    # 학습 배치 크기
minibatch_size = 4096       # 미니배치 크기
vf_loss_coeff = 0.01        # 가치 함수 손실 계수
fcnet_hiddens = [64, 64]    # 신경망 은닉층 크기
fcnet_activation = 'tanh'   # 활성화 함수
vf_share_layers = False     # 가치 함수 레이어 공유 여부
```

### 실험 대상 파라미터 (베이스라인 값)
```python
clip_param = 0.2            # PPO 클리핑 파라미터
vf_clip_param = 10.0        # 가치 함수 클리핑 파라미터
entropy_coeff = 0.0         # 엔트로피 계수
use_kl_loss = True          # KL 발산 손실 사용 여부
kl_coeff = 0.2              # KL 계수
kl_target = 0.01            # KL 목표값
grad_clip = None            # 그래디언트 클리핑
gamma = 0.99                # 할인 계수
use_gae = True              # GAE 사용 여부
use_critic = True           # Critic 사용 여부
```

**베이스라인 성능:**
- **최종 보상:** -307.88 ± 23.92
- **변동 계수 (CV):** -0.0777 (낮을수록 안정적)
- **SPS (Steps Per Second):** 969.12

## 3. 최고 성능 실험

### 3.1 성능 기준 (최종 보상)

## 3. Top Performing Experiments

### 3.1 성능 기준 (최종 보상)

| 순위 | 실험명 | 최종 보상 | 베이스라인 대비 | CV |
|------|------------|--------------|-------------|----|
| 1 | clip_aggressive | -290.94 ± 22.29 | +5.50% | -0.0766 |
| 2 | kl_disabled | -299.25 ± 21.55 | +2.80% | -0.0720 |
| 3 | kl_weak | -300.53 ± 20.66 | +2.39% | -0.0687 |
| 4 | grad_clip_tight | -302.04 ± 11.19 | +1.90% | -0.0371 |
| 5 | gamma_short | -302.64 ± 8.69 | +1.70% | -0.0287 |
| 6 | vf_clip_loose | -309.52 ± 14.73 | -0.53% | -0.0476 |
| 7 | entropy_minimal | -313.11 ± 16.05 | -1.70% | -0.0513 |
| 8 | gamma_long | -316.59 ± 22.38 | -2.83% | -0.0707 |
| 9 | grad_clip_loose | -317.17 ± 25.14 | -3.02% | -0.0793 |
| 10 | kl_strong | -317.73 ± 14.95 | -3.20% | -0.0470 |

### 3.2 안정성 기준 (낮은 변동 계수)

| 순위 | 실험명 | CV | 베이스라인 대비 | 최종 보상 |
|------|------------|-------|-------------|---------------|
| 1 | gamma_short | -0.0287 | +63.03% | -302.64 ± 8.69 |
| 2 | grad_clip_tight | -0.0371 | +52.21% | -302.04 ± 11.19 |
| 3 | vf_clip_tight | -0.0396 | +49.04% | -324.25 ± 12.84 |
| 4 | kl_strong | -0.0470 | +39.44% | -317.73 ± 14.95 |
| 5 | vf_clip_loose | -0.0476 | +38.74% | -309.52 ± 14.73 |
| 6 | entropy_minimal | -0.0513 | +33.99% | -313.11 ± 16.05 |
| 7 | entropy_medium | -0.0652 | +16.10% | -323.24 ± 21.07 |
| 8 | kl_weak | -0.0687 | +11.51% | -300.53 ± 20.66 |
| 9 | gamma_long | -0.0707 | +9.01% | -316.59 ± 22.38 |
| 10 | kl_disabled | -0.0720 | +7.28% | -299.25 ± 21.55 |

## 4. 파라미터 그룹별 분석

### 4.1 클리핑 파라미터 (Clipping)

**실험 구성:**
- `clip_conservative` (0.1): 보수적 정책 업데이트
- `clip_aggressive` (0.3): 공격적 정책 업데이트

| 실험명 | 최종 보상 | 베이스라인 대비 | CV | 베이스라인 대비 |
|------------|--------------|-------------|----|--------------|
| clip_aggressive | -290.94 ± 22.29 | **+5.50%** | -0.0766 | +1.36% |
| clip_conservative | -335.14 ± 24.26 | -8.85% | -0.0724 | +6.83% |

**분석:**
- `clip_aggressive` (0.3)가 **최고 성능**을 달성 (+5.50%)
- 더 큰 클리핑 범위가 HalfCheetah 환경에서 더 빠른 학습을 가능하게 함
- 안정성은 약간 감소하지만 성능 향상이 더 큼
- Schulman et al. (2017) 연구에 따르면 0.2가 권장되지만, 환경에 따라 조정 필요

### 4.2 엔트로피 계수 (Entropy)

**실험 구성:**
- `entropy_minimal` (0.001): 최소 탐험
- `entropy_medium` (0.01): 중간 탐험
- `entropy_high` (0.1): 높은 탐험

| 실험명 | 최종 보상 | 베이스라인 대비 | CV | 베이스라인 대비 |
|------------|--------------|-------------|----|--------------|
| entropy_minimal | -313.11 ± 16.05 | -1.70% | -0.0513 | +33.99% |
| entropy_medium | -323.24 ± 21.07 | -4.99% | -0.0652 | +16.10% |
| entropy_high | -362.69 ± 10.60 | -17.81% | -0.0292 | +62.42% |

**분석:**
- 엔트로피 계수가 높을수록 성능 저하가 심함
- 그러나 높은 엔트로피는 **매우 안정적인 학습** 제공 (CV: -0.0292)
- HalfCheetah는 결정론적 환경으로, 과도한 탐험이 불필요
- `entropy_minimal`이 성능과 안정성의 균형점

### 4.3 할인 계수 (Gamma)

**실험 구성:**
- `gamma_short` (0.95): 단기 보상 중시
- `gamma_long` (0.995): 장기 보상 중시

| 실험명 | 최종 보상 | 베이스라인 대비 | CV | 베이스라인 대비 |
|------------|--------------|-------------|----|--------------|
| gamma_short | -302.64 ± 8.69 | +1.70% | **-0.0287** | **+63.03%** |
| gamma_long | -316.59 ± 22.38 | -2.83% | -0.0707 | +9.01% |

**분석:**
- `gamma_short` (0.95)가 **가장 안정적** (CV: -0.0287)
- 단기 보상 중시가 HalfCheetah에서 더 효과적
- 연속적인 이동 태스크에서는 즉각적인 피드백이 중요
- 장기 할인은 분산을 증가시켜 불안정성 유발

### 4.4 그래디언트 클리핑 (Gradient Clipping)

**실험 구성:**
- `grad_clip_tight` (0.3): 강한 그래디언트 제한
- `grad_clip_loose` (1.0): 약한 그래디언트 제한

| 실험명 | 최종 보상 | 베이스라인 대비 | CV | 베이스라인 대비 |
|------------|--------------|-------------|----|--------------|
| grad_clip_tight | -302.04 ± 11.19 | +1.90% | -0.0371 | +52.21% |
| grad_clip_loose | -317.17 ± 25.14 | -3.02% | -0.0793 | -2.03% |

**분석:**
- `grad_clip_tight`이 성능과 안정성 모두 우수
- 강한 그래디언트 클리핑이 학습 안정성 크게 향상
- 느슨한 클리핑은 오히려 베이스라인보다 불안정
- PPO에서는 적절한 그래디언트 제한이 필수적

### 4.5 가치 함수 클리핑 (Value Function Clipping)

**실험 구성:**
- `vf_clip_tight` (1.0): 강한 가치 함수 제한
- `vf_clip_loose` (20.0): 약한 가치 함수 제한

| 실험명 | 최종 보상 | 베이스라인 대비 | CV | 베이스라인 대비 |
|------------|--------------|-------------|----|--------------|
| vf_clip_tight | -324.25 ± 12.84 | -5.31% | -0.0396 | +49.04% |
| vf_clip_loose | -309.52 ± 14.73 | -0.53% | -0.0476 | +38.74% |

**분석:**
- 두 설정 모두 베이스라인과 유사하거나 약간 낮은 성능
- 가치 함수 클리핑은 HalfCheetah에서 큰 영향 없음
- 기본값 (10.0)이 적절한 것으로 판단
- 안정성은 약간 향상되지만 성능 희생

### 4.6 KL 발산 손실 (KL Loss)

**실험 구성:**
- `kl_disabled` (use_kl_loss=False): KL 손실 비활성화
- `kl_weak` (kl_coeff=0.1): 약한 KL 제약
- `kl_strong` (kl_coeff=0.5): 강한 KL 제약

| 실험명 | 최종 보상 | 베이스라인 대비 | CV | 베이스라인 대비 |
|------------|--------------|-------------|----|--------------|
| kl_disabled | -299.25 ± 21.55 | **+2.80%** | -0.0720 | +7.28% |
| kl_weak | -300.53 ± 20.66 | +2.39% | -0.0687 | +11.51% |
| kl_strong | -317.73 ± 14.95 | -3.20% | -0.0470 | +39.44% |

**분석:**
- **KL 손실 비활성화가 최고 성능** (+2.80%)
- KL 제약이 없을 때 더 공격적인 정책 업데이트 가능
- 강한 KL 제약은 과도하게 보수적인 업데이트로 성능 저하
- PPO의 클리핑만으로도 충분한 안정성 확보 가능

### 4.7 GAE (Generalized Advantage Estimation)

**실험 구성:**
- `no_gae` (use_gae=False): GAE 비활성화 (단순 advantage)

**결과:**
- 실험 실패 (에피소드 truncation 오류)
- PPO에서 가치 함수 없이는 truncated 에피소드 처리 불가
- GAE는 PPO의 필수 구성 요소로 확인

## 5. 주요 발견 사항

### 5.1 최고 성능 구성

**실험:** clip_aggressive  
**최종 보상:** -290.94 ± 22.29  
**개선율:** +5.50%  
**파라미터 변경:** `clip_param = 0.3`

**특징:**
- 공격적인 정책 업데이트로 빠른 학습
- 적절한 안정성 유지 (CV: -0.0766)
- HalfCheetah의 연속 제어에 최적화

### 5.2 최고 안정성 구성

**실험:** gamma_short  
**변동 계수:** -0.0287  
**안정성 개선:** +63.03%  
**파라미터 변경:** `gamma = 0.95`

**특징:**
- 단기 보상 중시로 분산 최소화
- 일관된 학습 성능 (표준편차: 8.69)
- 성능도 베이스라인보다 우수 (+1.70%)

### 5.3 성능-안정성 균형

**최적 트레이드오프 후보:**
1. **clip_aggressive**: 최고 성능 + 적절한 안정성
2. **kl_disabled**: 높은 성능 + 양호한 안정성
3. **grad_clip_tight**: 좋은 성능 + 높은 안정성

## 6. 권장 사항

### 6.1 실전 적용 권장 설정

**시나리오 1: 최대 성능 추구**
```python
clip_param = 0.3              # clip_aggressive
use_kl_loss = False           # kl_disabled
gamma = 0.95                  # gamma_short
```
**예상 성능:** 베이스라인 대비 약 +7~10% 향상

**시나리오 2: 안정적 학습**
```python
gamma = 0.95                  # gamma_short
grad_clip = 0.3               # grad_clip_tight
entropy_coeff = 0.001         # entropy_minimal
```
**예상 안정성:** CV 50% 이상 개선

**시나리오 3: 균형잡힌 설정 (권장)**
```python
clip_param = 0.3              # 공격적 업데이트
gamma = 0.95                  # 단기 보상 중시
grad_clip = 0.3               # 안정적 그래디언트
use_kl_loss = False           # KL 제약 제거
entropy_coeff = 0.001         # 최소 탐험
```
**예상 효과:** 성능 +5~7%, 안정성 +40~50%

### 6.2 파라미터 조정 가이드

**조정 우선순위:**
1. **clip_param** (0.2 → 0.3): 가장 큰 성능 향상
2. **gamma** (0.99 → 0.95): 가장 큰 안정성 향상
3. **use_kl_loss** (True → False): 추가 성능 향상
4. **grad_clip** (None → 0.3): 안정성 보강
5. **entropy_coeff**: 환경에 따라 조정 (탐험 필요시만)

### 6.3 환경별 고려사항

**연속 제어 환경 (HalfCheetah, Walker, Hopper):**
- 공격적 클리핑 (0.3) 효과적
- 단기 할인 계수 (0.95) 권장
- 최소 엔트로피로 충분

**이산 행동 환경 (Atari, Grid World):**
- 표준 클리핑 (0.2) 유지
- 장기 할인 (0.99~0.995) 필요
- 적절한 엔트로피 (0.01) 중요

**고차원 상태 공간:**
- 그래디언트 클리핑 필수
- KL 제약 선택적 사용
- 안정성 우선 고려

## 7. 시각화

다음 그래프들이 생성되었습니다:
- `performance_vs_stability.png`: 성능 vs 안정성 트레이드오프 산점도
- `improvements_comparison.png`: 베이스라인 대비 개선율 막대 그래프
- `parameter_group_effects.png`: 파라미터 그룹별 성능 분포 박스플롯
- `learning_curves_comparison.png`: 학습 곡선 비교 (베이스라인 vs 상위 3개)

## 8. 결론

본 실험을 통해 PPO의 하이퍼파라미터가 HalfCheetah-v5 환경에서 성능과 안정성에 미치는 영향을 체계적으로 분석했습니다.

**핵심 결과:**
1. **clip_param=0.3**으로 설정 시 베이스라인 대비 **5.50% 성능 향상**
2. **gamma=0.95**로 조정 시 **63.03% 안정성 향상**
3. **KL 손실 비활성화**로 추가 **2.80% 성능 향상** 가능
4. 엔트로피 계수는 HalfCheetah에서 최소값 권장
5. 적절한 그래디언트 클리핑이 안정성에 중요

**실무 적용:**
- 단일 파라미터 변경만으로도 상당한 개선 가능
- 환경 특성에 맞는 파라미터 조정이 중요
- 성능과 안정성의 트레이드오프 고려 필요

**향후 연구:**
- 최적 파라미터 조합 탐색 (다중 파라미터 동시 변경)
- 다른 MuJoCo 환경에서의 일반화 검증
- 적응적 하이퍼파라미터 스케줄링 기법 연구

---

**실험 수행:** 2025년 11월 16일  
**소요 시간:** 약 3.6시간 (80회 실험)  
**환경:** AMD Ryzen 7 3700X (16 cores), RTX 3070 Ti (8GB), 15GB RAM

